# -*- coding: utf-8 -*-
"""UAS_DATA&ANALISIS_DANIEL_RAIHAN_BUNGA

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IZkdFJbIZ7MjPLzvrx4yZXTjzQKmHc0N

# **PREDICTIONS OF THE POTENTIAL OF SUPERHOST TO INCREASE GUEST SATISFACTION AND AIRBNB REVENUE**


Kelompok Data dan Analisis
1. Daniel Andrew Siahaan
2. Raihan
3. Bunga Lestari

## **ðŸ“ Table of Content ðŸ“**

- Data Cleansing/Preprocessing
    - Merge Dataset
    - Handling Invalid Values
    - Handling Duplicated Data
    - Handling Missing Values
    - Handling Outliers
    - Feature Engineering / Extraction
    - Feature Transformation (Numeric)
    - Feature Encoding (Categoric)
    - Data Splitting
    - Feature Selection
    - Handling Imbalanced Data
    
- Modelling & Evaluation
    - Machine Learning Techniques
    - Modelling
        1. Decision Tree
        2. Random Forest
        3. Logistic Regression
        4. Gaussian Naive Bayes
        5. K-Nearest Neighbor
        6. MLP Classifier (Neural Network)
        7. Adaboost Classifier
        8. Gradient Boosting Classifier
        9. XGBoost Classifier
        10. LGBM Classifier
    - Model Evaluation
    - Model Comparison
    - Model Selection
    - Business Insight and Recomendation
    - Strategic Action

- **Dataset**

[Open Data Airbnb Seattle](https://www.kaggle.com/datasets/airbnb/seattle)

Data PreProcessing

## **ðŸ“Œ Import Libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
import gdown
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import folium
import plotly.express as px
from folium import plugins
from folium.plugins import HeatMap
from scipy.stats import pointbiserialr

# Ignores any warning
import warnings
warnings.filterwarnings("ignore")

import matplotlib as mp
# %matplotlib inline
import textwrap
import matplotlib.ticker as mticker
import matplotlib.patches as mpatches
import matplotlib.gridspec as gridspec
from matplotlib.colors import LinearSegmentedColormap

"""## **ðŸ“Œ Import  Raw Datasets**"""

# Import Dataset
dataset_link = 'https://drive.google.com/drive/folders/1wLAk6IsUNCWhGENAmAUnmXZxLiiv7m4q?usp=sharing'
gdown.download_folder(dataset_link, quiet=True, use_cookies=False)

# Path Dataset
path = '/content/Dataset/'

# Get the data from Dataset Folder
csv_files = [os.path.join(path, file) for file in os.listdir(path) if file.endswith('.csv')]
df = {}

# Load each CSV file into a DataFrame
for file_name in csv_files:
    base_name = os.path.basename(file_name).replace('.csv', '')
    print(f"Load Path: {file_name} into DataFrame: {base_name}..")

    # Load the file and store it
    df[base_name] = pd.read_csv(file_name)

print("\nList Dataset")
print(df.keys())

reviews = pd.read_csv('/content/Dataset/reviews.csv')
calendar = pd.read_csv('/content/Dataset/calendar.csv')
listings = pd.read_csv('/content/Dataset/listings.csv')

print("Jumlah baris dan kolom:", reviews.shape)
df['reviews'].head()

print("Jumlah baris dan kolom:", calendar.shape)
df['calendar'].head()

print("Jumlah baris dan kolom:", listings.shape)
df['listings'].head()

df['listings'].columns

"""## **ðŸ“Œ Merge Datasets**"""

# Agregasi data reviews untuk menghitung jumlah ulasan per listing
reviews_agg = reviews.groupby('listing_id').agg({
    'id': 'count'  # Menghitung total ulasan per listing
}).rename(columns={'id': 'number_of_reviews'}).reset_index()
reviews_agg.head()

# Jika listings memiliki kolom 'id' dan bukan 'listing_id', ganti di penggabungan
merged1 = pd.merge(listings, reviews_agg, left_on='id', right_on='listing_id', how='left')

merged1.head()

# Ambil harga terbaru dari calendar
calendar['date'] = pd.to_datetime(calendar['date'])
latest_calendar = calendar.loc[calendar.groupby('listing_id')['date'].idxmax()]
latest_calendar.head()

# Mengkonversi type id 'listing_id' di latest_calendar seperti di merged_data
latest_calendar['listing_id'] = latest_calendar['listing_id'].astype(merged1['listing_id'].dtype)

# Merged data
df = pd.merge(merged1, latest_calendar[['listing_id', 'price', 'date']], on='listing_id', how='left')

df.head()

# Mengubah nama kolom
df = df.rename(columns={'price_x': 'price_listings'})
df = df.rename(columns={'price_y': 'price_calendar'})
df = df.rename(columns={'number_of_reviews_x': 'number_of_reviews'})

# Menghapus kolom double
df = df.drop(columns=['number_of_reviews_y'])

"""* Kolom price_x merupakan kolom price dari dataset listings yaitu harga sewa per malam (dalam dolar).
* Kolom price_y merupakan kolom price dari dataset calendar yaitu harga untuk tanggal tersebut (dalam dolar, bisa null untuk tanggal yang tidak tersedia).
"""

df.columns

#distribusi host_is_superhost
import plotly.express as px

host_superhost_counts = df['host_is_superhost'].value_counts()
host_superhost_percentage = (host_superhost_counts / host_superhost_counts.sum()) * 100

host_superhost_df = pd.DataFrame({
    'Host Superhost Status': host_superhost_counts.index,
    'Count': host_superhost_counts.values,
    'Percentage': host_superhost_percentage.values
})

fig = px.bar(
    host_superhost_df,
    x='Host Superhost Status',
    y='Count',
    color='Host Superhost Status', # Color the bars based on Superhost status
    text='Percentage',  # Display percentages on top of bars
    title='Distribution of Host Superhost Status',
    labels={'Count': 'Number of Hosts', 'Host Superhost Status': 'Host is Superhost', 'Percentage': 'Percentage of Hosts'}
)

fig.update_layout(title_x=0.5,  # Center the title
                  yaxis_title="Number of Hosts",
                  xaxis_title="Host is Superhost",
                  yaxis=dict(showgrid=True, gridcolor='lightgray'),
                  xaxis=dict(showgrid=False),
                  plot_bgcolor='white')

fig.update_traces(texttemplate='%{text:.2f}%', textposition='outside')

fig.show()

"""## **ðŸ“Œ Handling Invalid Values**"""

# Mengatur opsi untuk menampilkan semua kolom
pd.set_option('display.max_columns', None)  # Menampilkan semua kolom
display(df)

df.info()

"""### **1. Melakukan Penyesuaian Tipe Data Fitur `object` ke `datetime`**"""

# Mengubah kolom tanggal menjadi datetime
date_columns = ['last_scraped', 'host_since', 'calendar_last_scraped',
                'first_review', 'last_review', 'date']

for col in date_columns:
    df[col] = pd.to_datetime(df[col], errors='coerce')

"""Fitur yang diubah dari object ke datetime yaitu: last_scraped, host_since, calendar_last_scraped, first_review, last_review, dan date.

Karena, konversi ini memungkinkan analisis yang lebih efektif terhadap data berbasis waktu. Dengan tipe data datetime, kita dapat melakukan operasi seperti perhitungan durasi, pengelompokan data berdasarkan waktu, dan analisis tren temporal.

### **2. Melakukan Penyesuaian tipe data fitur `object` ke `float` beserta pengapusan simbol**
"""

# Mengubah kolom harga menjadi float
def convert_price(value):
    if pd.notnull(value):
        return float(value.replace('$', '').replace(',', ''))
    return value

columns_price = ['price_listings', 'weekly_price', 'monthly_price',
                 'security_deposit', 'cleaning_fee', 'extra_people', 'price_calendar']

for col in columns_price:
    df[col] = df[col].apply(convert_price)

"""Fitur yang diubah dari object ke float yaitu: price_listings, weekly_price, monthly_price, security_deposit, cleaning_fee, extra_people, dan price_calendar.

Karena konversi ini diperlukan untuk memastikan bahwa nilai-nilai yang berhubungan dengan harga dan biaya dapat digunakan dalam analisis numerik.

### **3. Melakukan Penyesuasian tipe data fitur `object` ke `bool`dari `t/f` menjadi `true / false`**
"""

# Mengubah nilai 't'/'f' menjadi True/False dan mengubah tipe data menjadi boolean
boolean_columns = ['host_is_superhost', 'host_has_profile_pic',
                   'host_identity_verified', 'is_location_exact',
                   'has_availability', 'instant_bookable',
                   'require_guest_profile_picture', 'require_guest_phone_verification']

for col in boolean_columns:
    df[col] = df[col].map({'t': True, 'f': False})
    df[col] = df[col].astype(bool)

"""Fitur yang diubah menjadi tipe data boolean yaitu: host_is_superhost, host_has_profile_pic, host_identity_verified, is_location_exact, has_availability, instant_bookable, require_guest_profile_picture, dan require_guest_phone_verification.

Karena konversi ini penting untuk memastikan bahwa kolom-kolom yang berisi informasi biner (ya/tidak) diwakili dengan tipe data yang sesuai.

### **4. Kolom `object` yang berisi angka perlu dikonversi ke `int` atau `float`**
"""

# Mengubah kolom menjadi int atau float
int_columns = ['accommodates', 'bathrooms', 'bedrooms', 'beds',
               'minimum_nights', 'maximum_nights', 'availability_30',
               'availability_60', 'availability_90', 'availability_365',
               'number_of_reviews', 'review_scores_rating', 'review_scores_accuracy',
               'review_scores_cleanliness', 'review_scores_checkin',
               'review_scores_communication', 'review_scores_location',
               'review_scores_value', 'calculated_host_listings_count']

float_columns = ['bathrooms', 'bedrooms', 'beds',
                 'review_scores_rating', 'review_scores_accuracy',
                 'review_scores_cleanliness', 'review_scores_checkin',
                 'review_scores_communication', 'review_scores_location',
                 'review_scores_value', 'reviews_per_month']

for col in int_columns:
    df[col] = pd.to_numeric(df[col], errors='coerce', downcast='integer')

for col in float_columns:
    df[col] = pd.to_numeric(df[col], errors='coerce', downcast='float')

"""konversi ini diperlukan untuk memastikan bahwa kolom-kolom yang berisi data numerik diwakili dengan tipe data yang sesuai, sehingga memungkinkan analisis matematis dan statistik yang lebih akurat.

### **5. Kolom `boolean` yang tidak cocok perlu dikonversi ke `object` atau `float`**
"""

# Mengubah kolom boolean menjadi object atau float
boolean1_columns = ['notes','neighbourhood']
for col in boolean1_columns:
    df[col] = df[col].astype(object)

boolean2_columns = ['price_calendar','cleaning_fee']
for col in boolean2_columns:
    df[col] = df[col].astype(float)

df.info()

df.head()

df.columns

"""## **ðŸ“Œ Handle Duplicated Data**"""

df[df.duplicated(keep=False)].sort_values(by=list(df.columns.values)).head()

#handle duplicated
df.duplicated().sum()

print(f"Data Frame Dimension Before Duplicate Removal: {df.shape}")
df = df.drop_duplicates().reset_index(drop=True)
print(f"Data Frame Dimension After Duplicate Removal: {df.shape}")

# Memeriksa baris duplikat
duplicate_rows = df.duplicated()

# Menampilkan jumlah baris duplikat
print(f"Jumlah baris duplikat: {duplicate_rows.sum()}")

# Menampilkan baris duplikat
if duplicate_rows.sum() > 0:
    print("Baris duplikat:")
    print(df[duplicate_rows])
else:
    print("Tidak ada baris duplikat.")

df.duplicated(subset=["id"]).sum()

"""### **Kesimpulan**

Tidak ditemui baris data yang memiliki duplikat, sehingga tidak perlu melakukan handling duplicated data. Selain itu, pada pengecekan duplikat subset untuk ID tidak ditemukan ada nya ID yang sama.
"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8, 6))

colors = ["#ffac18", "#c43623"]

custom_palette = sns.color_palette(colors)

ax = sns.barplot(
    x='host_is_superhost',
    y='review_scores_cleanliness',
    data=df,
    palette=custom_palette
)

plt.title('Distribusi Nilai Kebersihan Ulasan Berdasarkan Status Superhost', fontsize=14, fontweight='bold')
plt.xlabel('Status Superhost', fontsize=12)
plt.ylabel('Nilai Kebersihan', fontsize=12)

plt.xticks([0, 1], ['Host', 'Superhost'], fontsize=10)

plt.gca().invert_yaxis()

plt.grid(axis='y', linestyle='--', alpha=0.7)

sns.despine()

for p in ax.patches:
    ax.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),
                textcoords='offset points')

plt.tight_layout()
plt.show()

"""## **ðŸ“Œ Handle Missing Values**

---


"""

null_values = df.isnull().sum()
print(null_values)

# Menghitung total nilai null dan persentase
nvc = pd.DataFrame(null_values.sort_values(), columns=['Total Null Values'])
nvc['Percentage'] = (nvc['Total Null Values'] / df.shape[0]) * 100
nvc["Data Type"] = [df[col].dtype for col in df.columns]
nvc.sort_values(by=["Total Null Values", "Percentage"], ascending=False, inplace=True)

# Menampilkan hasil
nvc.style.background_gradient(cmap='Blues')

for x in df.columns:
    unq = list(df[x].unique())
    try:
        # sorted_unq = sorted(unq) #jangan gunakan sorted
        sorted_unq = unq
    except TypeError:
        sorted_unq = unq
    print(f'===== {x} =====')
    if len(sorted_unq) >= 50:
        print(sorted_unq[:50] + ['.....'])
    else:
        print(sorted_unq)
    print()

"""### **Drop Column**"""

# Menghapus kolom license dan square feet
df = df.drop(columns=['license', 'square_feet'])

# Menghapus kolom neighbourhood
df = df.drop(columns=['neighbourhood'])

"""* Kolom license dan square feet terlalu banyak baris yang kosong, membuat kolom tersebut tidak bisa digunakan
* Kolom neighbourhood tidak diperlukan karena sudah mempunyai versi cleansed nya

### **Drop Row**
"""

# Menghapus nilai null pada id
df = df.dropna(subset=['listing_id'])

"""Kolom id tidak boleh memiliki nilai kosong

### **Imputation Numeric**
"""

# Mencari Skewness pada fitur yang terdapat pada price_columns
price_columns = ['price_calendar', 'price_listings', 'weekly_price', 'monthly_price', 'security_deposit', 'cleaning_fee']

skewness_results = []
for col in price_columns:
    skewness = df[col].skew()
    if skewness > 0.5:
        skew_description = "Right Skewed"
    elif skewness < -0.5:
        skew_description = "Left Skewed"
    else:
        skew_description = "Approximately Symmetrical"
    skewness_results.append([col, skewness, skew_description])

skewness_df = pd.DataFrame(skewness_results, columns=['Column', 'Skewness', 'Skewness Description'])
skewness_df

# Mengisi nilai kosong dengan nilai sebelumnya
rate_columns = ['last_review', 'first_review', 'host_response_rate', 'host_response_time']
for col in rate_columns:
    df[col] = df[col].ffill()

# Mengisi nilai kosong untuk kolom harga
price_columns = ['price_calendar']
for col in price_columns:
    df[col] = df[col].replace({'\$': '', ',': ''}, regex=True).astype(float)
    df[col] = df[col].fillna(df[col].median())

# Mengisi nilai kosong untuk monthly price
df['monthly_price'] = df.apply(
    lambda row: row['price_listings'] * 30 if pd.isna(row['monthly_price']) else row['monthly_price'],
    axis=1
)

# Mengisi nilai kosong untuk weekly price
df['weekly_price'] = df.apply(
    lambda row: row['price_listings'] * 7 if pd.isna(row['weekly_price']) else row['weekly_price'],
    axis=1
)

# Mengisi nilai kosong dengan 0
score_columns = ['reviews_per_month', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness',
                  'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value',
                  'security_deposit', 'cleaning_fee']
for col in score_columns:
    df[col] = df[col].fillna(0)

"""Mengisi nilai null pada kolom numerik:
* fitur yang terdapat pada `rate_columns` nilai null Diisi dengan nilai sebelumnya
* fitur yang terdapat pada `price_columns` nilai null Diisi dengan nilai `median` karena memiliki right skewed dan nilai outlier ekstrem.
* fitur `monthly_price` nilai null Diisi dengan mengkalikan harga listing perhari dengan jumlah hari per bulan
* fitur `weekly_price` nilai null Diisi dengan mengkalikan harga listing perhari dengan jumlah hari per minggu
* fitur yang terdapat pada `score_columns` Diisi dengan nilai 0 karena tidak semua listing memiliki review dan fee tambahan

### **Imputation Categoric**
"""

# Mengisi nilai kosong dengan kata terbanyak
frequent_text = ['space', 'host_neighbourhood', 'bathrooms', 'host_location', 'bedrooms', 'host_since',
                 'host_total_listings_count', 'host_listings_count', 'host_name', 'property_type', 'beds', 'host_acceptance_rate']
for col in frequent_text:
    most_frequent_text = df[col].mode()[0]
    df[col] = df[col].fillna(most_frequent_text)

# Mengisi nilai kosong dengan tidak tersedia
not_available = df[['notes', 'neighborhood_overview', 'transit', 'host_about', 'summary', 'zipcode']] = df[['notes', 'neighborhood_overview', 'transit', 'host_about', 'summary', 'zipcode']].fillna('not available')

# Mengisi nilai kosong gambar dengan no image
no_image = df[['thumbnail_url', 'medium_url', 'xl_picture_url', 'host_picture_url', 'host_has_profile_pic', 'host_thumbnail_url']] = df[['thumbnail_url', 'medium_url', 'xl_picture_url', 'host_picture_url', 'host_has_profile_pic', 'host_thumbnail_url']].fillna("No Image")

"""Mengisi nilai null pada kolom kategorik:
* fitur yang terdapat pada `frequent_text` Diisi dengan kata yang paling banyak muncul atau modus
* fitur yang terdapat pada `not_available` Diisi dengan kata tidak tersedia
* fitur yang terdapat pada `no_image` Diisi dengan kata no image untuk kolom yang memiliki gambar
"""

# Menghitung jumlah nilai null di setiap kolom setelah preprocessing
null_values_after = df.isnull().sum()

# Membuat DataFrame untuk menampilkan hasil
nvc = pd.DataFrame(null_values_after.sort_values(), columns=['Total Null Values'])
nvc['Percentage'] = (nvc['Total Null Values'] / df.shape[0]) * 100
nvc["Data Type"] = [df[col].dtype for col in df.columns]
nvc.sort_values(by=["Total Null Values", "Percentage"], ascending=False, inplace=True)

# Menampilkan hasil
nvc.style.background_gradient(cmap='Blues')

"""### **Kesimpulan**
Proses penanganan nilai hilang dalam dataset ini melibatkan beberapa langkah yang sistematis:

1. Penghapusan Fitur: Fitur-fitur seperti 'license', 'square_feet', dan 'neighbourhood' dihapus karena tingginya jumlah nilai hilang atau karena redundansi, di mana fitur 'neighbourhood' telah digantikan oleh fitur lain.
2. Penghapusan Baris: Baris yang memiliki nilai hilang pada fitur 'listing_id' dihapus, mengingat pentingnya pengidentifikasi ini dalam analisis.

3. Imputasi (Numerik):

* Fitur yang merepresentasikan tingkat (last_review, first_review, host_response_rate, host_response_time) diimputasi menggunakan metode forward fill untuk menjaga konsistensi temporal.
* Fitur terkait harga dibersihkan dengan menghapus simbol mata uang dan diimputasi menggunakan nilai median, mengingat adanya skewness ke kanan dan outlier ekstrem.
* Fitur terkait harga perbulan dan perminggu diisi dengan perkalian harga perharinya.
* Fitur review score diisi dengan 0 agar setiap listing mendapatkan score yang fair dan fitur fee tambahan diisi dengan 0 karena tidak semua listing punya fasilitas tambahan.
4. Imputasi (Kategorikal):
* Fitur yang berisi data teks diimputasi dengan kategori yang paling sering muncul (modus).
* Fitur tertentu seperti 'host_name' dan 'host_identity_verified' diisi dengan nilai 'not available'.
* Fitur yang berkaitan dengan gambar diisi dengan "No Image.

Setelah melakukan langkah-langkah tersebut, tidak ada nilai hilang yang tersisa dalam dataset. Pemilihan metode imputasi (mean, median, modus, forward fill, atau konstanta) didasarkan pada tipe data dan distribusi fitur yang bersangkutan.

## **ðŸ“Œ Handling Outliers**

### **Checking Outlier**
"""

import plotly.graph_objects as go

#Visualisasi Outlier untuk semua fitur dengan membedakan total superhost dengan not superhost
def plot_outliers(df, col):
  fig = go.Figure()

  # Plot all data
  fig.add_trace(go.Box(y=df[col], name='All Data', marker_color='lightblue'))

  # Plot superhost data
  superhost_df = df[df['host_is_superhost'] == True]
  fig.add_trace(go.Box(y=superhost_df[col], name='Superhost', marker_color='green'))

  # Plot non-superhost data
  not_superhost_df = df[df['host_is_superhost'] == False]
  fig.add_trace(go.Box(y=not_superhost_df[col], name='Not Superhost', marker_color='red'))


  fig.update_layout(
      title=f'Outlier Detection for {col}',
      title_x=0.5,
      yaxis_title=col,
      xaxis_title="Host Type",
      plot_bgcolor='white',
  )
  fig.show()

#
num_cols = df.select_dtypes(include=np.number).columns.tolist()

for col in num_cols:
  plot_outliers(df, col)

num_cols = df.select_dtypes(include=np.number).columns.tolist()  # Or define your num_cols manually

print(f'Jumlah baris: {len(df)}')
outlier = []
no_outlier = []
is_outlier = []
low_lim = []
high_lim = []
filtered_entries = np.array([True] * len(df))

for col in num_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    low_limit = Q1 - (IQR * 1.5)
    high_limit = Q3 + (IQR * 1.5)
    filter_outlier = ((df[col] >= low_limit) & (df[col] <= high_limit))
    outlier.append(len(df[~filter_outlier]))
    no_outlier.append(len(df[filter_outlier]))
    is_outlier.append(df[col][~filter_outlier].any())
    low_lim.append(low_limit)
    high_lim.append(high_limit)
    filtered_entries = ((df[col] >= low_limit) & (df[col] <= high_limit)) & filtered_entries

print("Outlier All Data :", len(df[~filtered_entries]))
print("Not Outlier All Data :", len(df[filtered_entries]))
print()

outlier_summary = pd.DataFrame({
    "Column Name": num_cols,
    "is Outlier": is_outlier,
    "Lower Limit": low_lim,
    "Upper Limit": high_lim,
    "Outlier": outlier,
    "No Outlier": no_outlier
})

outlier_summary

# Select numerical columns
numerical_cols = df.select_dtypes(include=np.number).columns

# Create an empty dictionary to store distribution descriptions
distribution_descriptions = {}

# Iterate through numerical columns
for col in numerical_cols:
    # Calculate skewness
    skewness = df[col].skew()

    # Categorize distribution based on skewness
    if abs(skewness) < 0.5:
        distribution = "Approximately Normal"
    elif skewness > 0.5:
        distribution = "Right Skewed"
    else:
        distribution = "Left Skewed"

    # Store the description
    distribution_descriptions[col] = distribution

# Create a DataFrame from the distribution descriptions
distribution_df = pd.DataFrame(list(distribution_descriptions.items()), columns=['Feature', 'Distribution'])

# Display the DataFrame
distribution_df

"""### **Cara Handle Outlier**

### **IQR (Interquartile Range)**
"""

# Define a function to handle outliers using IQR
# def handle_outliers_iqr(df, col):
    # # Calculate Q1, Q3, and IQR
    # Q1 = df[col].quantile(0.25)
    # Q3 = df[col].quantile(0.75)
    # IQR = Q3 - Q1

    ## Calculate lower and upper limits
    # lower_limit = Q1 - 1.5 * IQR
    # upper_limit = Q3 + 1.5 * IQR

    ## Replace outliers with the limits
    # df[col] = np.where(df[col] < lower_limit, lower_limit, df[col]) # Replace outliers below lower limit
    # df[col] = np.where(df[col] > upper_limit, upper_limit, df[col]) # Replace outliers above upper limit

    # return df

## Apply IQR outlier handling to specified numerical columns
# num_cols = df.select_dtypes(include=np.number).columns.tolist() # Get numerical column names

# for col in num_cols:
    # df = handle_outliers_iqr(df, col) # Apply IQR handling to each numerical column

## Recalculate outlier statistics after handling
# outlier = []
# no_outlier = []
# is_outlier = []
# low_lim = []
# high_lim = []
# filtered_entries = np.array([True] * len(df)) # Initialize a boolean array for filtering

# for col in num_cols:
    # Q1 = df[col].quantile(0.25) # Calculate Q1
    # Q3 = df[col].quantile(0.75) # Calculate Q3
    # IQR = Q3 - Q1 # Calculate IQR
    # low_limit = Q1 - (IQR * 1.5) # Calculate lower limit
    # high_limit = Q3 + (IQR * 1.5) # Calculate upper limit
    # filter_outlier = ((df[col] >= low_limit) & (df[col] <= high_limit)) # Create a boolean filter for outliers
    # outlier.append(len(df[~filter_outlier])) # Count outliers
    # no_outlier.append(len(df[filter_outlier]))  # Count non-outliers
    # is_outlier.append(df[col][~filter_outlier].any()) # Check if any outliers exist
    # low_lim.append(low_limit)  # Store lower limit
    # high_lim.append(high_limit)  # Store upper limit
    # filtered_entries = ((df[col] >= low_limit) & (df[col] <= high_limit)) & filtered_entries # Update filter for all columns

# print("Outlier All Data :", len(df[~filtered_entries])) # Total outliers across all columns
# print("Not Outlier All Data :", len(df[filtered_entries])) # Total non-outliers across all columns
# print()

# outlier_summary = pd.DataFrame({
    # "Column Name": num_cols,
    # "is Outlier": is_outlier,
    # "Lower Limit": low_lim,
    # "Upper Limit": high_lim,
    # "Outlier": outlier,
    # "No Outlier": no_outlier
# })

# Display the outlier summary
# outlier_summary

"""* Tidak ada outlier pada dataset ini untuk semua kolom. Semua data dianggap berada dalam batas yang wajar sesuai dengan perhitungan IQR

* Meskipun nilai outlier teratasi semua, Perhitungan IQR tidak cocok untuk mendeteksi outlier dalam dataset dengan distribusi tertentu, terutama jika distribusi tidak normal atau data cenderung homogen.

Maka pada dataset yang kita miliki tidak terlalu cocok karna banyak memiliki nilai ekstrem.

### **Z-Score**
Z-Score digunakan untuk kolom yang mendekati distribusi normal.
"""

# Z-Score outlier handling
# from scipy import stats

# def handle_outliers_zscore(df, col, threshold=3):
    # z = np.abs(stats.zscore(df[col]))
    # df[col] = np.where(z > threshold, np.nan, df[col])
    # return df

# num_cols = df.select_dtypes(include=np.number).columns.tolist()
# for col in num_cols:
    # df = handle_outliers_zscore(df, col)

# Impute missing values after Z-score outlier removal (replace NaN with median)
# for col in num_cols:
    # df[col] = df[col].fillna(df[col].median())

# Recalculate outlier statistics after Z-score handling
# outlier = []
# no_outlier = []
# is_outlier = []
# low_lim = []
# high_lim = []
# filtered_entries = np.array([True] * len(df))

# for col in num_cols:
    # Q1 = df[col].quantile(0.25)
    # Q3 = df[col].quantile(0.75)
    # IQR = Q3 - Q1
    # low_limit = Q1 - (IQR * 1.5)
    # high_limit = Q3 + (IQR * 1.5)
    # filter_outlier = ((df[col] >= low_limit) & (df[col] <= high_limit))
    # outlier.append(len(df[~filter_outlier]))
    # no_outlier.append(len(df[filter_outlier]))
    # is_outlier.append(df[col][~filter_outlier].any())
    # low_lim.append(low_limit)
    # high_lim.append(high_limit)
    # filtered_entries = ((df[col] >= low_limit) & (df[col] <= high_limit)) & filtered_entries

# print("Outlier All Data :", len(df[~filtered_entries]))
# print("Not Outlier All Data :", len(df[filtered_entries]))
# print()

# outlier_summary = pd.DataFrame({
    # "Column Name": num_cols,
    # "is Outlier": is_outlier,
    # "Lower Limit": low_lim,
    # "Upper Limit": high_lim,
    # "Outlier": outlier,
    # "No Outlier": no_outlier
# })

# outlier_summary

"""* Sebelum handling dengan Z-Score, jumlah total baris adalah 3191.
* Setelah Z-Score, terdapat 2841 outliers dan hanya 350 non-outliers yang tersisa.

* Z-Score terlalu ketat untuk dataset dengan distribusi skewed, sehingga menganggap banyak data valid sebagai outlier.


Hasil Z-Score kurang optimal untuk dataset ini karena terlalu banyak data yang dianggap outlier (89%)

### **Winsorization**

Winsorization mengganti nilai outlier dengan nilai batas tertentu, misalnya persentil 1% (batas bawah) dan 99% (batas atas). Ini cocok untuk data yang benar-benar memiliki outlier ekstrem.
"""

from scipy.stats.mstats import winsorize

# Select numerical columns for winsorization
num_cols = df.select_dtypes(include=np.number).columns

# Apply winsorization to specified numerical columns
for col in num_cols:
    df[col] = winsorize(df[col], limits=[0.01, 0.01])  # Winsorize at 1st and 99th percentiles

# Recalculate outlier statistics after winsorization
outlier = []
no_outlier = []
is_outlier = []
low_lim = []
high_lim = []
filtered_entries = np.array([True] * len(df))

for col in num_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    low_limit = Q1 - (IQR * 1.5)
    high_limit = Q3 + (IQR * 1.5)
    filter_outlier = ((df[col] >= low_limit) & (df[col] <= high_limit))
    outlier.append(len(df[~filter_outlier]))
    no_outlier.append(len(df[filter_outlier]))
    is_outlier.append(df[col][~filter_outlier].any())
    low_lim.append(low_limit)
    high_lim.append(high_limit)
    filtered_entries = ((df[col] >= low_limit) & (df[col] <= high_limit)) & filtered_entries

print("Outlier All Data :", len(df[~filtered_entries]))
print("Not Outlier All Data :", len(df[filtered_entries]))
print()

outlier_summary = pd.DataFrame({
    "Column Name": num_cols,
    "is Outlier": is_outlier,
    "Lower Limit": low_lim,
    "Upper Limit": high_lim,
    "Outlier": outlier,
    "No Outlier": no_outlier
})

# Display the outlier summary after winsorization
outlier_summary

"""* Outlier All Data: 2755 outlier, dengan 436 data yang tidak terdeteksi sebagai outlier.
* Menjaga Data: Winsorization menggantikan nilai ekstrim dengan batas atas atau bawah yang telah ditentukan, bukan menghapusnya, sehingga data tetap utuh dan tidak hilang.

Winsorization dapat diteruskan jika Anda ingin menjaga sebanyak mungkin data sambil mengurangi dampak dari nilai ekstrim.

### **Kesimpulan**

* `IQR`, Meskipun nilai outlier teratasi semua, Perhitungan IQR tidak cocok untuk mendeteksi outlier dalam dataset dengan distribusi tertentu, terutama jika distribusi tidak normal atau data cenderung homogen.
* `Winsorization` dapat diteruskan jika Anda ingin menjaga sebanyak mungkin data sambil mengurangi dampak dari nilai ekstrim.
* `Z-Score` lebih cocok jika data terdistribusi normal.

1. `Pilihan Pertama`, Karena Data kita memiliki banyak nilai ekstrem dan tidak terdistribusi normal: Winsorization adalah pilihan yang baik karena menggantikan outlier dengan batas yang lebih realistis tanpa menghapus data, namun tetap dapat mempertahankan informasi penting dalam data.

Namun `jika hasilnya model akhir kurang memuaskan` bisa memilih opsi kedua

2. `Pilihan Kedua`, mencari kembali metode yang baik selain `winsorization`

## **ðŸ“Œ Feature Engineering / Extraction**

## **Menambah Kolom / Fitur**
"""

# Memisahkan kolom date menjadi day, month, dan year
df['day'] = df.date.dt.day
df['month'] = df.date.dt.month
df['year'] = df.date.dt.year

"""### **1. Membuat Kolom `day, month, and year`**

Memisahkan kolom date menjadi hari, tanggal, dan tahun untuk mendapatkan insight yang lebih rinci
"""

review_score_columns = [
    'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness',
    'review_scores_checkin', 'review_scores_communication',
    'review_scores_location', 'review_scores_value'
]
df['average_review_score'] = df[review_score_columns].mean(axis=1)

"""### **2. Membuat Kolom `average review score`**

Rata-rata dari semua skor ulasan
"""

df['property_attractiveness'] = (df['average_review_score'] * df['number_of_reviews'])

"""### **3. Membuat Kolom `property attractiveness`**

Mengukur seberapa menarik properti berdasarkan ulasan dan skor rata-rata
"""

df['review_positive_ratio'] = (df['review_scores_rating'] / 100) * df['number_of_reviews']

"""### **4. Membuat Kolom `review positive ratio`**

Mengukur kualitas ulasan berdasarkan skor ulasan keseluruhan
"""

df['booking_frequency'] = 1 - (df['availability_365'] / 365)

"""### **5. Membuat Kolom `booking frequency`**

Mengukur apakah properti sering dipesan atau tersedia untuk waktu yang lama
"""

df['high_quality_listings_ratio'] = (
    (df['review_scores_rating'] >= 90).astype(int) / df['number_of_reviews']
).fillna(0)

"""### **6. Membuat Kolom `high quality listings`**

Mengukur seberapa sering listing mendapatkan skor ulasan tinggi
"""

df['response_speed'] = df['number_of_reviews'] / df['reviews_per_month']

"""### **7. Membuat Kolom `response speed`**

Mengukur seberapa sering listing mendapatkan review tiap bulannya
"""

display(df)

"""## **ðŸ“Œ Feature Transformation (Numeric)**"""

numerical_cols = df.select_dtypes(include=np.number).columns

# Create an empty dictionary to store skewness and transformation suggestions
transformation_suggestions = {}

# Iterate through numerical columns
for col in numerical_cols:
    skewness = df[col].skew()

    if abs(skewness) < 0.5:
        transformation = "(Approximately Normal)"
    elif skewness > 0.5:
        transformation = "(Right Skewed)"
    else:  # skewness < -0.5
        transformation = "(Left Skewed)"

    transformation_suggestions[col] = {
        'skewness': skewness,
        'transformation': transformation
    }

# Create a DataFrame from the transformation suggestions
transformation_df = pd.DataFrame.from_dict(transformation_suggestions, orient='index')
transformation_df = transformation_df.reset_index().rename(columns={'index': 'Feature'})

# Display the DataFrame
transformation_df

from scipy.stats import skew, kurtosis
import pandas as pd

num_cols = df.select_dtypes(include=np.number).columns.tolist()

skew_type_list = []
skew_val_list = []
kurtosis_val_list = []

for column in num_cols:
    data = df[column].dropna(axis=0)
    mean = round(data.mean(), 3)
    median = data.median()
    mode = data.mode()[0]
    skew_val = round(skew(data, nan_policy="omit"), 3)
    kurtosis_val = round(kurtosis(data, nan_policy="omit"), 3)

    if (mean == median == mode) or (-0.2 < skew_val < 0.2):
        skew_type = "Normal Distribution (Symmetric)"
    elif mean < median < mode:
        skew_type = "Negatively Skewed"
        if skew_val <= -1:
            skew_type = "Highly Negatively Skewed"
        elif -0.5 >= skew_val > -1:
            skew_type = "Moderately Negatively Skewed"
        else:
            skew_type = "Moderately Normal Distribution (Symmetric)"
    else:
        skew_type = "Positively Skewed"
        if skew_val >= 1:
            skew_type = "Highly Positively Skewed"
        elif 0.5 <= skew_val < 1:
            skew_type = "Moderately Positively Skewed"
        else:
            skew_type = "Moderately Normal Distribution (Symmetric)"

    skew_type_list.append(skew_type)
    skew_val_list.append(skew_val)
    kurtosis_val_list.append(kurtosis_val)

dist = pd.DataFrame({
    "Column Name": num_cols,
    "Skewness": skew_val_list,
    "Kurtosis": kurtosis_val_list,
    "Type of Distribution": skew_type_list
})

# Specific adjustments for certain columns (if needed)
dist.loc[dist["Column Name"].isin(["Z_CostContact", "Z_Revenue"]), "Type of Distribution"] = "Uniform Distribution"
dist.loc[dist["Column Name"].isin(["Kidhome", "Teenhome"]), "Type of Distribution"] = "Bimodal Distribution"

dist = dist.sort_values(["Type of Distribution", "Column Name"]).reset_index(drop=True)
dist

"""Dari hasil temuan tersebut, kita dapat menentukan beberapa transformasi yang akan dilakukan berdasarkan tipe distribusi setiap kolom. Berikut adalah langkah-langkah transformasi yang bisa diambil:

 **`1. Scaling and Converting to a Normal Distribution`**


Kolom dengan skewness tinggi (baik positif maupun negatif) memerlukan transformasi untuk mendekati distribusi normal. Transformasi yang umum digunakan adalah:

* Log Transformation: Untuk kolom yang sangat positif skewed.
* Box-Cox Transformation: Memerlukan data positif dan dapat mengubah data menjadi distribusi normal.
* Yeo-Johnson Transformation: Alternatif * Box-Cox untuk data yang bisa memiliki nilai nol atau negatif.

Daftar kolom yang dapat ditransformasi:

- average_review_score
- review_scores_rating
- accommodates
- bathrooms
- bedrooms
- beds
- calculated_host_listings_count
- cleaning_fee
- extra_people
- guests_included
- high_quality_listings_ratio
- host_listings_count
- host_total_listings_count
- minimum_nights
- monthly_price
- number_of_reviews
- price_calendar
- price_listings
- property_attractiveness
- response_speed
- review_positive_ratio
- reviews_per_month
- security_deposit
- weekly_price
- availability_365
- availability_60
- availability_90
- maximum_nights
- review_scores_accuracy
- review_scores_checkin
- review_scores_cleanliness
- review_scores_communication
- review_scores_location
- review_scores_value

**`2. Just Scaling`**

Kolom dengan distribusi yang relatif normal atau sedikit skewed mungkin hanya memerlukan skala ulang, seperti:

Normalization: Mengubah nilai ke dalam rentang [0, 1].
Standardization: Mengubah nilai ke bentuk distribusi standar dengan mean 0 dan standar deviasi 1.

Daftar kolom yang dapat diskala:

- availability_30
- id
- latitude
- listing_id
- longitude
- booking_frequency
- host_id

**` 3. Kolom yang Tidak Memerlukan Transformasi`**

Kolom yang tidak memiliki nilai skewness atau kurtosis yang signifikan mungkin tidak memerlukan transformasi apapun, seperti:

- day
- month
- scrape_id
- year

**` 4. Choice Determination`**


Pada proses Feature Transformation / Scaling ini kita menggunakan `Yeo-Johnson Transformation` pada kolom-kolom yang masih memiliki skala yang besar, karena dari hasilnya kita bisa melihat hasil bentuk curve yang lebih Normal Distribusi. Dan sangat cocok untuk penggunaan Algoritma berbasis tree.

"""

exclude = []  # Create an empty list to store columns to exclude

log_cols = sorted(list(dist[
    dist["Type of Distribution"].str.contains("Positively Skewed") &
    ~dist["Column Name"].isin(exclude)
]["Column Name"].values))

norm_cols = sorted(list(dist[
    dist["Type of Distribution"].str.contains("Normal Distribution") &
    ~dist["Column Name"].isin(exclude)
]["Column Name"].values))

print("Log Transformation =", log_cols)
print("Normalisasi/Standardization =", norm_cols)

"""### **Yeo-Johnson Transformation** (Increase the accuracy 2%)"""

import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import yeojohnson, boxcox

# Assuming 'df' is your DataFrame and 'log_cols' is defined
log_cols = ['average_review_score', 'review_scores_rating', 'accommodates', 'bathrooms',
    'bedrooms', 'calculated_host_listings_count', 'cleaning_fee', 'extra_people',
    'guests_included', 'high_quality_listings_ratio', 'host_listings_count',
    'host_total_listings_count', 'minimum_nights', 'monthly_price', 'number_of_reviews',
    'price_calendar', 'price_listings', 'property_attractiveness', 'response_speed',
    'review_positive_ratio', 'reviews_per_month', 'security_deposit', 'weekly_price']

fig, ax = plt.subplots(len(log_cols), 2, figsize=(15, 30))

for i in range(0, len(log_cols)):
    # Yeo-Johnson Transformation
    data, fitted_lambda = yeojohnson(df[log_cols[i]], lmbda=None)
    df[log_cols[i]] = data # Update the column in the DataFrame

    kde1 = sns.kdeplot(df[log_cols[i]], ax=ax[i][0])
    kde2 = sns.kdeplot(data, ax=ax[i][1])
    kde2.set_ylabel(None)
    plt.tight_layout()

"""### **RobustScaler** (For Outlier)"""

from sklearn.preprocessing import RobustScaler

# Initialize the RobustScaler
scaler = RobustScaler()

# Fit and transform the numerical features
df[num_cols] = scaler.fit_transform(df[num_cols])

# Display the scaled DataFrame
df[num_cols]

nan_counts = df[num_cols].isna().sum()

print(nan_counts)

has_nan = df[num_cols].isna().any().any()

if has_nan:
    print("Ada nilai NaN yang hadir di kolom numerik yang ditentukan")
else:
    print("Tidak ada nilai NaN yang ditemukan di kolom numerik yang ditentukan.")

"""### **Distribution Column**

"""

fig, ax = plt.subplots(len(log_cols),2,figsize=(15,30))
for i in range(0,len(log_cols)):
    kde1 = sns.kdeplot(df[log_cols[i]], ax=ax[i][0])
    kde2 = sns.kdeplot(np.log(df[log_cols[i]]+1), ax=ax[i][1])
    kde2.set_ylabel(None)
    plt.tight_layout()

"""### **Kesimpulan**

Berdasarkan evaluasi terhadap beberapa fitur yang telah diproses menggunakan teknik scaling dan transformation sebelumnya, dapat disimpulkan bahwa distribusi nilai skewnessnya kini lebih seragam dan tidak menunjukkan variasi yang signifikan. Dengan demikian, dapat dikatakan bahwa penerapan teknik scaling dan transformation pada fitur yang telah dilakukan sudah tepat dan efektif.

## **ðŸ“Œ Feature Encoding (Categoric)**
"""

for x in df.columns:
    unq = list(df[x].unique())
    try:
        # sorted_unq = sorted(unq) #jangan gunakan sorted
        sorted_unq = unq
    except TypeError:
        sorted_unq = unq
    print(f'===== {x} =====')
    if len(sorted_unq) >= 10:
        print(sorted_unq[:10] + ['.....'])
    else:
        print(sorted_unq)
    print()

"""### **Label Encoding**"""

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder

categorical_cols = df.select_dtypes(include=['object', 'category','bool']).columns
display(categorical_cols)
label_encoders = {}

for col in categorical_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

df.head()

selected_features_25 = [
    'review_scores_cleanliness',  # Kebersihan
    'review_scores_communication',  # Komunikasi
    'host_response_time',  # Waktu respons
    'review_scores_location',  # Lokasi
    'host_identity_verified',  # Verifikasi identitas
    'monthly_price',  # Harga bulanan
    'require_guest_phone_verification',  # Verifikasi telepon tamu
    'amenities',  # Fasilitas
    'property_type',  # Jenis properti
    'neighbourhood_cleansed',  # Lokasi spesifik
    'review_scores_value',  # Nilai yang baik
    'review_scores_checkin',  # Proses check-in
    'host_response_rate',  # Tingkat respons
    'space',  # Deskripsi ruang
    'calendar_updated',  # Pembaruan kalender
    'zipcode',  # Kode pos
    'weekly_price',  # Harga mingguan
    'cleaning_fee',  # Biaya pembersihan
    'guests_included',  # Jumlah tamu yang termasuk
    'bedrooms',  # Jumlah kamar tidur
    'latitude',  # Lokasi geografis
    'longitude',  # Lokasi geografis
    'cancellation_policy',  # Kebijakan pembatalan
    'extra_people',  # Biaya tambahan untuk tamu ekstra
    'review_scores_accuracy',  # Akurasi deskripsi
]

"""### **Kesimpulan**

Berdasarkan hasil verifikasi fitur-fitur yang telah diproses menggunakan teknik encoding sebelumnya, diketahui bahwa semua nilai telah berubah menjadi numerik sesuai dengan yang ditentukan. Dengan demikian, dapat disimpulkan bahwa metode fitur encoding yang telah kami terapkan sudah valid dan akurat.
"""

df.head()

"""## **ðŸ“Œ Data Splitting**"""

from sklearn.model_selection import train_test_split

# splitting tha data
df_train, df_test = train_test_split(df, test_size=0.20, stratify=df[['host_is_superhost']], random_state=42)
df_train.reset_index(drop=True, inplace=True)
df_test.reset_index(drop=True, inplace=True)

print(df_train.shape)
print(df_test.shape)

"""## **ðŸ“Œ Feature Selection**

Dalam proses pemilihan fitur, kami menerapkan beberapa metode di mana fitur terbaik yang dihasilkan akan digabungkan dengan metode berikutnya. Kami kemudian mengidentifikasi fitur-fitur yang sudah ada sebelumnya dan menambahkan fitur-fitur yang belum termasuk dalam fitur terbaik tersebut.

### **1. Drop Unnecessary Feature**
"""

df_train.info()

df_train.drop(['id', 'listing_url', 'host_url', 'listing_id', 'host_id', 'street', 'notes', 'picture_url', 'xl_picture_url', 'host_picture_url','host_thumbnail_url', 'neighborhood_overview', 'transit','host_location','host_about','host_neighbourhood',
               'host_verifications', 'scrape_id', 'last_scraped', 'name', 'thumbnail_url', 'medium_url', 'host_name', 'experiences_offered','host_since','date', 'calendar_last_scraped', 'first_review', 'last_review' ], axis=1, inplace=True)

df_test.drop(['id', 'listing_url', 'host_url', 'listing_id', 'host_id', 'street', 'notes', 'picture_url', 'xl_picture_url', 'host_picture_url','host_thumbnail_url', 'neighborhood_overview', 'transit','host_location','host_about','host_neighbourhood',
               'host_verifications', 'scrape_id', 'last_scraped', 'name', 'thumbnail_url', 'medium_url', 'host_name', 'experiences_offered','host_since','date', 'calendar_last_scraped', 'first_review', 'last_review'], axis=1, inplace=True)

"""* Informasi URL dan Identitas: Kolom seperti listing_url, host_url, dan ID tidak relevan karena tidak berkontribusi pada analisis atau pembelajaran model.

* Informasi Visual dan Deskriptif: Kolom seperti picture_url, host_picture_url, dan neighborhood_overview yang bersifat informatif tetapi tidak terstruktur untuk analisis.

* Informasi Waktu dan Riwayat: Kolom seperti last_scraped, host_since, first_review, dan last_review dihapus untuk menyederhanakan dataset.

* Atribut Tambahan Lainnya: Kolom seperti name, notes, host_name, dan experiences_offered juga dihilangkan karena kurang relevan dalam konteks prediksi.
"""

df_train

"""Hasil fitur berkurang dari 101 kolom menjadi 72 kolom

### **2. Univariate Selection**
"""

# define X and y
X_train = df_train.drop(['host_is_superhost'], axis=1) #features
y_train = df_train['host_is_superhost'] #target

"""- #### **ANOVA F-value**

`ANOVA F-value` memperkirakan derajat linearitas antara fitur input (yaitu, fitur independen) dan fitur output (yaitu, fitur dependen). Nilai F yang tinggi menunjukkan derajat linearitas yang tinggi dan nilai F yang rendah menunjukkan derajat linearitas yang rendah.

Scikit-learn menyediakan dua fungsi untuk menghitung nilai F:

1. `sklearn.feature_selection.f_regression` for regression problems
2. `sklearn.feature_selection.f_classif` for classification problems

Kekurangan:

Nilai F ANOVA hanya menangkap hubungan linear antara fitur input dan fitur output.|
"""

from sklearn.feature_selection import f_classif
feature_names = X_train.columns

# create f_classif object
f_value = f_classif(X_train, y_train)

# print the name and F-value of each feature
# for feature in zip(feature_names, f_value[0]):
#     print(feature)

fs = pd.DataFrame({
    "feature_names":feature_names,
    "f_value":f_value[0]
}).sort_values("f_value", ascending=False)

# Create a bar chart
plt.figure(figsize=(10,4))
plt.bar(data=fs, x="feature_names", height="f_value", color="orange")
plt.xticks(rotation="vertical")
plt.ylabel("F-value")
plt.title("F-value Comparison")
plt.show()

"""* Mengambil 20 top feature"""

feature_importance = []
for i in fs["feature_names"].values[:20]:
    if i not in feature_importance:
        feature_importance.append(i)
feature_importance

"""Untuk metode awal kita mengambil 20 fitur terbaik.

- #### **Variance Threshold**

Variance Threshold menghapus fitur-fitur yang variansinya di bawah nilai `threshold` yang telah ditentukan. Secara default, ini menghapus semua fitur dengan varians nol, yaitu fitur yang memiliki nilai yang sama di semua sampel.

Keuntungan:

Ini dapat digunakan untuk pembelajaran tanpa pengawasan (unsupervised learning).

Kekurangan:

Variance Threshold hanya mempertimbangkan hubungan antar fitur, tetapi tidak mempertimbangkan hubungan antara fitur input dengan fitur output.
"""

# import VarianceThreshold
from sklearn.feature_selection import VarianceThreshold
# create VarianceThreshold object
selector = VarianceThreshold(threshold=0.0)
# train and transform
selector.fit_transform(X_train)
# print the name and variance of each feature
# for feature in zip(feature_names, selector.variances_):
#     print(feature)

fs = pd.DataFrame({
    "feature_names":feature_names,
    "variances":selector.variances_
}).sort_values("variances", ascending=False)

# Create a bar chart
plt.figure(figsize=(10,4))
plt.bar(data=fs, x="feature_names", height="variances", color="orange")
plt.xticks(rotation="vertical")
plt.ylabel("Variance")
plt.title("Variance Comparison")
plt.show()

for i in fs["feature_names"].values[:20]:
    if i not in feature_importance:
        feature_importance.append(i)
feature_importance

"""penambahan terlihat setelah **'require_guest_profile_picture'**

- #### **Mutual information**

Informasi mutual (MI) mengukur ketergantungan satu variabel terhadap variabel lainnya dengan mengkuantifikasi jumlah informasi yang diperoleh tentang satu fitur melalui fitur lainnya. MI bersifat simetris dan tidak negatif, serta sama dengan nol jika dan hanya jika dua variabel acak independen, dan nilai yang lebih tinggi menunjukkan ketergantungan yang lebih tinggi.

Scikit-learn menyediakan dua fungsi untuk menghitung nilai MI:

- `sklearn.feature_selection.mutual_info_regression` for regression problems
- `sklearn.feature_selection.mutual_info_classif` for classification problems

Keuntungan:

MI dapat menangkap hubungan non-linear antara fitur input dan fitur output.
"""

# import mutual_info_classif
from sklearn.feature_selection import mutual_info_classif
# create mutual_info_classif object
MI_score = mutual_info_classif(X_train, y_train, random_state=0)
# Print the name and mutual information score of each feature
# for feature in zip(feature_names, MI_score):
#     print(feature)

fs = pd.DataFrame({
    "feature_names":feature_names,
    "MI_score":MI_score
}).sort_values("MI_score", ascending=False)

# Create a bar chart
plt.figure(figsize=(10,4))
plt.bar(data=fs, x="feature_names", height="MI_score", color="orange")
plt.xticks(rotation="vertical")
plt.ylabel("Mutual Information Score")
plt.title("Mutual Information Score Comparison")
plt.show()

for i in fs["feature_names"].values[:20]:
    if i not in feature_importance:
        feature_importance.append(i)
feature_importance

"""Dalam metode Mutual information dari visualisasi grafik banyak fitur yang memiliki nilai terbaik, oleh karena itu kita hanya mengambil 10 fitur saja.
Penambahan terlihat setelah fitur **'property_type'**. Ada 2 fitur tambahan.

- #### **Scikit-learnâ€™s SelectKBest**

SelectKBest memilih fitur-fitur menggunakan sebuah fungsi (dalam hal ini nilai F ANOVA) dan kemudian "menghapus semua kecuali k fitur dengan skor tertinggi".

Uji statistik dapat digunakan untuk memilih fitur-fitur yang memiliki hubungan terkuat dengan variabel output. `Mutual information, ANOVA F-test dan chi square` adalah beberapa metode paling populer untuk Univariate Feature Selection.

---

Jika transformasi data tidak memungkinkan karena suatu alasan (misalnya, nilai negatif adalah faktor penting), maka kita harus memilih statistik lain untuk menilai fitur kita:

```
sklearn.feature_selection.f_classif computes ANOVA f-value
sklearn.feature_selection.mutual_info_classif
```
"""

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2, f_classif, mutual_info_classif

# apply SelectKBest class to extract top 10 best features

# computes chi2
# semuanya harus positif
# bestfeatures = SelectKBest(score_func=chi2, k=10)

# computes ANOVA f-value
bestfeatures = SelectKBest(score_func=f_classif, k=10)

fit = bestfeatures.fit(X_train, y_train)
dfscores = pd.DataFrame(fit.scores_)
dfcolumns = pd.DataFrame(X_train.columns)
#concat two dataframes for better visualization
featureScores = pd.concat([dfcolumns,dfscores],axis=1)
featureScores.columns = ['Specs','Score']  #naming the dataframe columns
featureScores.sort_values('Score', ascending=False, inplace=True)
print(featureScores.nlargest(10,'Score'))  #print 10 best features

# Create a bar chart
plt.figure(figsize=(10,4))
plt.bar(data=featureScores, x="Specs", height="Score", color="orange")
plt.xticks(rotation="vertical")
plt.ylabel("SelectKBest Score")
plt.title("SelectKBest Score Comparison")
plt.show()

for i in featureScores["Specs"].values[:20]:
    if i not in feature_importance:
        feature_importance.append(i)
feature_importance

df_train[feature_importance]

"""Menghasilkan 8 fitur tambahan.

- #### **WOE**

Weight of Evidence (WOE) adalah metode yang digunakan untuk mengukur seberapa kuat hubungan antara variabel independen kategorikal (prediktor) dengan variabel target yang bersifat biner (0 atau 1). WOE bekerja dengan cara menghitung logaritma dari rasio peluang (odds ratio) antara jumlah kejadian positif (1) dan negatif (0) dalam setiap kategori variabel independen. Dengan kata lain, WOE membantu memahami seberapa baik suatu kategori dalam variabel prediktor dapat memengaruhi atau memprediksi hasil superhost (1) atau bukan superhost (0) pada variabel target.
"""

# data = df_train  # Menggunakan data df_train
# TARGET_COLUMN = 'host_is_superhost'  # Target kolom untuk WOE

# # Fungsi untuk menghitung WOE secara manual
# def calculate_woe(data, feature, target):
#     temp = data[[feature, target]].copy()
#     temp['total'] = 1

#     # Membuat tabel agregasi
#     agg = temp.groupby(feature).agg({
#         target: 'sum',  # Jumlah positif (target = 1)
#         'total': 'count'  # Total observasi di setiap grup
#     }).reset_index()

#     # Menambahkan jumlah negatif (target = 0)
#     agg['negative'] = agg['total'] - agg[target]

#     # Menghindari pembagian nol dengan menambahkan epsilon kecil
#     agg['positive_dist'] = agg[target] / agg[target].sum()
#     agg['negative_dist'] = agg['negative'] / agg['negative'].sum()

#     # Menghitung WOE
#     agg['woe'] = np.log((agg['positive_dist'] + 1e-10) / (agg['negative_dist'] + 1e-10))

#     # Menghindari nilai WOE yang tidak valid (misalnya, inf)
#     agg['woe'].replace([np.inf, -np.inf], 0, inplace=True)

#     return agg[[feature, 'woe']]

# # Menghitung WOE untuk setiap fitur dalam data kecuali target
# columns_to_transform = [col for col in data.columns if col != TARGET_COLUMN]
# woe_results = {}

# for column in columns_to_transform:
#     # Periksa tipe data dan binning untuk data numerik
#     if data[column].dtype in ['int64', 'float64']:
#         # Binning fitur numerik dengan kuantil
#         data[column] = pd.qcut(data[column], q=5, duplicates='drop', labels=False)
#     if data[column].dtype in ['int64', 'float64', 'object']:
#         try:
#             woe_df = calculate_woe(data, column, TARGET_COLUMN)
#             woe_results[column] = woe_df
#         except Exception as e:
#             print(f"Skipping WOE calculation for '{column}' due to error: {e}")

# # Menampilkan kolom berdasarkan total nilai WOE terbesar hingga terkecil
# woe_totals = {}
# for column, woe_df in woe_results.items():
#     total_woe = woe_df['woe'].abs().sum()  # Menggunakan nilai absolut dari WOE
#     woe_totals[column] = total_woe

# # Urutkan berdasarkan total WOE
# sorted_columns = sorted(woe_totals.items(), key=lambda x: x[1], reverse=True)

# # Membuat DataFrame hasil WOE
# woe_totals_df = pd.DataFrame(sorted_columns, columns=['Column', 'Total WOE'])

# # Menampilkan fitur teratas (20 fitur dengan WOE terbesar)
# top_20_woe_features = woe_totals_df.head(20)
# print("\n20 Fitur dengan Total WOE Terbesar:")
# top_20_woe_features

# # Membuat barchart
# plt.figure(figsize=(10, 4))
# plt.bar(data=top_20_woe_features, x="Column", height="Total WOE", color="orange")
# plt.xticks(rotation="vertical")
# plt.ylabel("Total WOE", fontsize=12)
# plt.title("Top 20 Features Based on Total WOE", fontsize=14)
# plt.show()

# for i in top_20_woe_features["Column"].values[:20]:
#     if i not in feature_importance:
#         feature_importance.append(i)
# feature_importance

# df_train[feature_importance]

"""Hasil dari Woe Method terdapat 7 fitur tambahan

### **3. Feature Importance**

Kita dapat memperoleh tingkat kepentingan setiap fitur dalam dataset dengan menggunakan properti feature importance dari model yang digunakan.

Feature importance memberikan skor untuk setiap fitur dalam data. Semakin tinggi skor yang diberikan, semakin penting atau relevan fitur tersebut terhadap variabel target (output).

Feature importance merupakan kelas bawaan yang tersedia pada algoritma berbasis pohon keputusan (Tree-Based Classifiers). Dalam hal ini, kita akan menggunakan algoritma Extra Trees Classifier untuk mengekstraksi 10 fitur teratas dari dataset.
"""

from sklearn.ensemble import ExtraTreesClassifier

model = ExtraTreesClassifier()
model.fit(X_train,y_train)
print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers
#plot graph of feature importances for better visualization
feat_importances = pd.Series(model.feature_importances_, index=X_train.columns)
feat_importances = pd.DataFrame(feat_importances).reset_index(names="feature_names") \
    .rename(columns={0:"Score"}) \
    .sort_values("Score", ascending=False).reset_index(drop=True)
feat_importances.nlargest(10, "Score").plot(x="feature_names", y="Score", kind='barh')
plt.show()

for i in feat_importances["feature_names"].values[:20]:
    if i not in feature_importance:
        feature_importance.append(i)
feature_importance

"""### **4. Correlation Matrix with Heatmap**

Definisi Korelasi:
Korelasi menunjukkan bagaimana suatu fitur saling berhubungan satu sama lain atau terhadap variabel target. Korelasi dapat bersifat:

* Positif: Peningkatan nilai pada satu fitur menyebabkan peningkatan nilai pada variabel target.

* Negatif: Peningkatan nilai pada satu fitur menyebabkan penurunan nilai pada variabel target.

* Visualisasi dengan Heatmap:
Heatmap membantu kita mengidentifikasi fitur mana yang memiliki hubungan paling kuat dengan variabel target. Untuk itu, kita akan memvisualisasikan korelasi antar fitur menggunakan heatmap dari library Seaborn.

* Mengatasi Redundansi Antar Fitur:
Dalam analisis korelasi antar fitur, penting untuk memeriksa adanya fitur yang saling redundan (memiliki korelasi tinggi satu sama lain). Dalam kasus tersebut, salah satu fitur dapat dihapus, terutama fitur dengan korelasi rendah terhadap variabel target (response). Hal ini bertujuan untuk mengurangi multikolinearitas dan meningkatkan efisiensi model.
"""

plt.figure(figsize=(30,20))
corr = df_train.corr(numeric_only=True)
mask = np.triu(np.ones_like(corr, dtype=np.bool_))
sns.heatmap(corr, cmap='Blues', annot=True, fmt='.2f', mask=mask)

"""**Checking Correlation with Target host_is_superhost**"""

corr = df_train.corrwith(df_train["host_is_superhost"], numeric_only=True)
corr = corr.reset_index(name='corr value')
corr["Corr Type"] = corr["corr value"].apply(lambda x : "Positif" if x >= 0 else "Negatif")
corr["corr value"] = corr["corr value"].apply(lambda x : abs(x))
corr = corr.sort_values('corr value', ascending=False, ignore_index=True)
corr.head(10)

corr = df_train.corrwith(df_train["host_is_superhost"], numeric_only=True)
corr = corr.reset_index(name='corr value')
corr = corr.sort_values('corr value', ascending=False)[1:]

plt.figure(figsize=(20, 8))
ax = sns.barplot(x='index', y="corr value", data=corr, order=corr["index"], color='orange')
for p in ax.patches:
    ax.annotate(
        f'{p.get_height():0.2f}',
        (p.get_x() + p.get_width() / 2., p.get_height()),
        ha = 'center',
        va = 'center',
        xytext = (0, 10),
        fontsize=10,
        textcoords = 'offset points')
plt.xticks(rotation=90)
plt.show()

target = "host_is_superhost"
high_corr_cols = [i for i in list(corr[corr["corr value"] > 0.15]["index"].values) if i != target]
print(high_corr_cols)

"""#### Mengambil 20 top feature yang memiliki nilai terbaik"""

for i in corr["index"].values[:20]:
    if i not in feature_importance:
        feature_importance.append(i)
feature_importance

"""### **5. Multicollinearity Check (Drop Redundancy)**

Definisi Multikolinearitas:
Multikolinearitas terjadi ketika variabel independen dalam model saling berkorelasi. Kondisi ini dapat:

* Mengurangi kinerja model.
* Menurunkan signifikansi statistik dari variabel independen, sehingga sulit untuk menentukan pengaruh individual suatu fitur terhadap variabel target.


Pemeriksaan Redundansi Fitur:
Dari fitur-fitur yang telah dipilih (hasil gabungan Top 20 Features), kita akan melakukan pengecekan ulang untuk mendeteksi redundansi antar fitur.

Fitur-fitur dengan korelasi antar fitur yang tinggi (di atas ambang batas, misalnya `threshold > 0.70`) akan diidentifikasi.
Selanjutnya, fitur-fitur tersebut akan dibandingkan korelasinya terhadap variabel target. Salah satu fitur dengan korelasi lebih rendah terhadap target akan dihapus untuk mengurangi redundansi.

#### **1. Correlation Coefficient**
"""

plt.figure(figsize=(30,20))
corr = df_train[feature_importance+["host_is_superhost"]].corr(numeric_only=True)
mask = np.triu(np.ones_like(corr, dtype=np.bool_))
sns.heatmap(corr, cmap='Blues', annot=True, fmt='.2f', mask=mask)

"""**Manampilkan Korelasi Feature > Threshold 0.70**

Akan ada info mengenai Feature apa saja yang perlu di drop
"""

def corrtarget(x):
    target = "host_is_superhost"
    return df_train[x].corr(df_train[target])

def corrresp(x):
    target = "host_is_superhost"
    col1 = x["A"]
    col2 = x["B"]

    cor1 = df_train[col1].corr(df_train[target])
    cor2 = df_train[col2].corr(df_train[target])

    if cor1 < cor2:
        return col1
    else:
        return col2
    return col1

corr_matrix = df_train[feature_importance].corr()
target = "host_is_superhost"

# Flatten correlation matrix.
flat_cm = corr_matrix.stack().reset_index()
flat_cm.columns = ['A', 'B', 'correlation']
flat_cm = flat_cm.loc[flat_cm.correlation < 1, :]
flat_cm = flat_cm.sort_values("correlation", ascending=False)
redundan = flat_cm[flat_cm["correlation"] >= 0.7].reset_index(drop=True)
redundan['A vs Target'] = redundan['A'].apply(lambda x: corrtarget(x))
redundan['B vs Target'] = redundan['B'].apply(lambda x: corrtarget(x))
redundan = redundan.drop_duplicates(subset=["correlation"])
redundan["drop"] = redundan.apply(corrresp, axis=1)
redundan

"""#### **2. VIF (Variance Inflation Factor)**

* Tujuan VIF:
VIF digunakan untuk mendeteksi multikolinearitas dengan menunjukkan sejauh mana varians dari koefisien regresi meningkat akibat adanya korelasi antar prediktor.

* Interpretasi Nilai VIF:

- VIF = 1: Tidak ada korelasi antara fitur dengan prediktor lainnya (kondisi ideal).

- VIF 1 hingga 5: Menunjukkan korelasi yang moderat, masih dapat diterima.

- VIF > 5 atau 10: Menunjukkan tingkat multikolinearitas yang tinggi, memerlukan evaluasi lebih lanjut atau penghapusan fitur.

* Dampak VIF Tinggi:
Nilai VIF yang tinggi mengurangi signifikansi statistik dari variabel independen, sehingga sulit untuk menentukan kontribusi individu dari suatu fitur terhadap model. Oleh karena itu, fitur dengan VIF tinggi perlu dipertimbangkan untuk dihapus guna meningkatkan stabilitas model.
"""

# calculate VIF scores for each feature
from statsmodels.stats.outliers_influence import variance_inflation_factor as vif
from statsmodels.tools.tools import add_constant

X = add_constant(df_train[feature_importance])

vif_df = pd.DataFrame([vif(X.values, i)
               for i in range(X.shape[1])],
              index=X.columns).reset_index()
vif_df.columns = ['feature','vif_score']
vif_df = vif_df.loc[vif_df.feature!='const']
vif_df['vif_score'] = round(vif_df['vif_score'], 4)
vif_df.sort_values("vif_score", ascending=False, inplace=True)
vif_df

# prompt: cek nilai nan vif_df ada berapa jumlah nya

# Assuming vif_df is already defined as in your provided code.
nan_count = vif_df['vif_score'].isnull().sum()
print(f"Number of NaN values in vif_df['vif_score']: {nan_count}")

"""### **Drop Redundant Features**"""

from statsmodels.stats.outliers_influence import variance_inflation_factor as vif
from statsmodels.tools.tools import add_constant

# Assuming vif_df is already calculated as shown in your provided code

features_to_drop = vif_df[vif_df['vif_score'] > 5]['feature'].tolist()

# Only drop features from feature_importance list
features_to_drop = [feature for feature in features_to_drop if feature in feature_importance]

# Remove redundant features
feature_importance = [feature for feature in feature_importance if feature not in features_to_drop]

print(f"Features dropped due to high VIF: {features_to_drop}")
print(f"Updated feature_importance list: {feature_importance}")

"""### **Kesimpulan**

Berdasarkan hasil analisis dan seleksi fitur yang telah dilakukan melalui berbagai teknik feature selection, termasuk:

* Penghapusan Fitur yang Tidak Relevan (Drop Unnecessary Features)

* Seleksi Univariat, meliputi:
 * ANOVA F-value
 * Variance Threshold
 * Mutual Information
 * SelectKBest

* Feature Importance Analysis
* Korelasi Pearson (Pearson Correlation)
* Pemeriksaan Multikolinearitas (Multicollinearity Check) dan Penghapusan Redundansi (Drop Redundancy)

Fitur-fitur yang terpilih untuk digunakan dalam proses pemodelan telah ditentukan sebagai berikut:
"""

df_train[feature_importance]

"""## **Pemilihan fitur untuk Modelling**
Berdasarkan hasil dari berbagai metode seleksi fitur, kami telah melakukan seleksi tambahan yang disesuaikan dengan kebutuhan bisnis. Fitur yang paling representatif terhadap aspek-aspek utama dalam bisnis ini akan kami pilih untuk digunakan dalam tahap pemodelan.

### **10 Features**
"""

selected_features = [
    'review_scores_cleanliness',  # Kebersihan
    'review_scores_communication',  # Komunikasi
    'host_response_time',  # Waktu respons
    'review_scores_location',  # Lokasi
    'host_identity_verified',  # Verifikasi identitas
    'monthly_price',  # Harga bulanan
    'require_guest_phone_verification',  # Verifikasi telepon tamu
    'amenities',  # Fasilitas
    'property_type',  # Jenis properti
    'neighbourhood_cleansed'  # Lokasi spesifik
]

# Memilih fitur dari df_train
df_10_features = df_train[selected_features]

df_10_features

"""**Pemilihan 10 fitur ini berdasarkan `Kualitas Pengalaman Tamu dan Kepercayaan Host`**

### **15 Features**
"""

selected_features_15 = [
    'review_scores_cleanliness',  # Kebersihan
    'review_scores_communication',  # Komunikasi
    'host_response_time',  # Waktu respons
    'review_scores_location',  # Lokasi
    'host_identity_verified',  # Verifikasi identitas
    'monthly_price',  # Harga bulanan
    'require_guest_phone_verification',  # Verifikasi telepon tamu
    'amenities',  # Fasilitas
    'property_type',  # Jenis properti
    'neighbourhood_cleansed',  # Lokasi spesifik
    'review_scores_value',  # Nilai yang baik
    'review_scores_checkin',  # Proses check-in
    'host_response_rate',  # Tingkat respons
    'space',  # Deskripsi ruang
    'calendar_updated'  # Pembaruan kalender
]

# Memilih fitur dari df_train
df_15_features = df_train[selected_features_15]
df_15_features

"""**Pemilihan 15 fitur ini berdasarkan `Kualitas Pengalaman Tamu dan Manajemen Host`**

### **23 Features**
"""

selected_features_23 = [
    'review_scores_cleanliness',  # Kebersihan
    'review_scores_communication',  # Komunikasi
    'host_response_time',  # Waktu respons
    'review_scores_location',  # Lokasi
    'host_identity_verified',  # Verifikasi identitas
    'monthly_price',  # Harga bulanan
    'require_guest_phone_verification',  # Verifikasi telepon tamu
    'amenities',  # Fasilitas
    'property_type',  # Jenis properti
    'neighbourhood_cleansed',  # Lokasi spesifik
    'review_scores_value',  # Nilai yang baik
    'review_scores_checkin',  # Proses check-in
    'host_response_rate',  # Tingkat respons
    'space',  # Deskripsi ruang
    'calendar_updated',  # Pembaruan kalender
    'zipcode',  # Kode pos
    'weekly_price',  # Harga mingguan
    'cleaning_fee',  # Biaya pembersihan
    'guests_included',  # Jumlah tamu yang termasuk
    'bedrooms',  # Jumlah kamar tidur
    'cancellation_policy',  # Kebijakan pembatalan
    'extra_people',  # Biaya tambahan untuk tamu ekstra
    'review_scores_accuracy',  # Akurasi deskripsi
]

"""**Pemilihan 23 fitur ini berdasarkan `Kualitas Pengalaman Tamu, Manajemen Host, dan Karakteristik Properti (tanpa bujur lintang)`**

### **25 Features**
"""

selected_features_25 = [
    'review_scores_cleanliness',  # Kebersihan
    'review_scores_communication',  # Komunikasi
    'host_response_time',  # Waktu respons
    'review_scores_location',  # Lokasi
    'host_identity_verified',  # Verifikasi identitas
    'monthly_price',  # Harga bulanan
    'require_guest_phone_verification',  # Verifikasi telepon tamu
    'amenities',  # Fasilitas
    'property_type',  # Jenis properti
    'neighbourhood_cleansed',  # Lokasi spesifik
    'review_scores_value',  # Nilai yang baik
    'review_scores_checkin',  # Proses check-in
    'host_response_rate',  # Tingkat respons
    'space',  # Deskripsi ruang
    'calendar_updated',  # Pembaruan kalender
    'zipcode',  # Kode pos
    'weekly_price',  # Harga mingguan
    'cleaning_fee',  # Biaya pembersihan
    'guests_included',  # Jumlah tamu yang termasuk
    'bedrooms',  # Jumlah kamar tidur
    'latitude',  # Lokasi geografis
    'longitude',  # Lokasi geografis
    'cancellation_policy',  # Kebijakan pembatalan
    'extra_people',  # Biaya tambahan untuk tamu ekstra
    'review_scores_accuracy',  # Akurasi deskripsi
]
# Memilih fitur dari df_train
df_25_features = df_train[selected_features_25]
df_25_features

"""**Pemilihan 25 fitur ini berdasarkan `Kualitas Pengalaman Tamu, Manajemen Host, dan Karakteristik Properti`**

Dengan pemilihan tiga kategori tersebut, kami akan memilih 25 fitur yang dianggap representatif, karena kategori-kategori ini mencakup aspek-aspek yang krusial untuk kebutuhan bisnis ini.

## **ðŸ“Œ Handling Imbalanced Data**

### **Data Splitting**
"""

from sklearn.model_selection import train_test_split

# df_train, df_test = train_test_split(df, test_size=0.20, stratify=df[['host_is_superhost']], random_state=42)

X = df.drop('host_is_superhost', axis=1)[selected_features_23].reset_index(drop=True) # Features
y = df['host_is_superhost'].reset_index(drop=True) # Target

# Split into train/test (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2,
    random_state=42,  # Untuk reproducibility agar hasil tetap konsisten
)

print("Training shape:", X_train.shape, y_train.shape)
print("Testing shape:", X_test.shape, y_test.shape)
print("X shape", X.shape)
print("y shape", y.shape)

fig = plt.figure(figsize = (10, 5))

plt.subplot(121)
plt.pie(y_train.value_counts(),
        labels = ['No Superhost', 'Superhost'],
        autopct = '%.1f%%',
        radius = 1,
        colors=["#f5b042", "#f55742"],
        textprops={'fontsize': 10, 'fontweight': 'bold'})
plt.title('Superhost Outcome Pie Chart', fontsize = 10, fontweight = 'bold')

plt.subplot(122)
resp = y_train.apply(lambda x: "No Superhost" if x == 0 else "Superhost")
t = sns.countplot(x=resp, palette=["#f5b042", "#f55742"])
t.set_xlabel('host_is_superhost', fontweight = 'bold', fontsize = 10)
t.set_ylabel('Count', fontweight = 'bold', fontsize = 10)

plt.title('Superhost Outcome Distributions', fontsize = 10, fontweight = 'bold')
plt.tight_layout()

"""### **RandomOverSampler Method**"""

from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import RandomOverSampler, SMOTE

print('Before OverSampling, the shape of X_train: {}'.format(X_train.shape))
print('Before OverSampling, the shape of y_train: {} \n'.format(y_train.shape))

print("Before OverSampling, counts of label '1': {}".format(sum(y_train == 1))) # Response
print("Before OverSampling, counts of label '0': {} \n".format(sum(y_train == 0))) # No Response

# Undersampling
# us = RandomUnderSampler(sampling_strategy = 'auto')
# X_balanced_res, y_balanced_res = us.fit_resample(X_train,y_train)

# Oversampling
os = RandomOverSampler(sampling_strategy = 0.5, random_state = 2)
X_balanced_res, y_balanced_res = os.fit_resample(X_train,y_train)

# # Oversampling SMOTE
# sm = SMOTE(sampling_strategy=0.5, random_state = 2)
# X_balanced_res, y_balanced_res = sm.fit_resample(X_train,y_train)

print('After OverSampling, the shape of X_train: {}'.format(X_balanced_res.shape))
print('After OverSampling, the shape of y_train: {} \n'.format(y_balanced_res.shape))

print("After OverSampling, counts of label '1': {}".format(sum(y_balanced_res == 1)))
print("After OverSampling, counts of label '0': {}".format(sum(y_balanced_res == 0)))

X_train = X_balanced_res
y_train = y_balanced_res

print("Training shape:", X_train.shape, y_train.shape)
print("Testing shape:", X_test.shape, y_test.shape)

"""**ML Modelling & Evaluation**

Best Model:

Sources Parameter: https://www.geeksforgeeks.org/how-to-tune-hyperparameters-in-gradient-boosting-algorithm/

## **ðŸ“Œ Machine Learning Techniques**

Untuk memprediksi kelas adalah superhost atau tidak kita menggunakan binary classification untuk mendeteksinya berdasarkan beberapa fitur yang sudah kita gunakan

source: https://www.researchgate.net/profile/Vedant-Bahel/publication/347580959_A_Comparative_Study_on_Various_Binary_Classification_Algorithms_and_their_Improved_Variant_for_Optimal_Performance/links/60911b01299bf1ad8d7727a0/A-Comparative-Study-on-Various-Binary-Classification-Algorithms-and-their-Improved-Variant-for-Optimal-Performance.pdf


**Modelling** menggunakan percobaan model:
1. Decision Tree
2. Random Forest
3. Logistic Regression
4. Gaussian Naive Bayes
5. K-Nearest Neighbor
6. MLP Classifier (Neural Network)
7. Adaboost Classifier
8. XGBoost Classifier
9. LGBM Classifier
10. Gradient Boosting Classifier
11. Support Vector Machine

Apabila **overfitting** menggunakan metode:
1. cross-validation (2f, 4f, 5f and 10f)

**Hyperparameter tuning** yang digunakan:
1. Grid Search (Hasil menyebabkan overfitting)

**Model evaluasi** yang digunakan metrik:
1. Accuracy
2. Precision
3. Recall
4. F1-score

**Target Explanation**

> * `True Positive (TP)`, ketika seorang host yang sebenarnya adalah superhost diprediksi sebagai superhost.
> * `False Positive (FP)`, ketika seorang host yang sebenarnya bukan superhost tetapi diprediksi sebagai superhost.
> * `True Negative (TN)`, ketika seorang host yang sebenarnya bukan superhost diprediksi sebagai bukan superhost.
> * `False Negative (FN), ketika seorang host yang sebenarnya adalah superhost tetapi diprediksi sebagai bukan superhost.

> * `Positive / 1` = Host yang teridentifikasi sebagai superhost.
> * `Negative / 0`= Host yang teridentifikasi sebagai bukan superhost.

> * `False Negative` = Host yang teridentifikasi sebagai bukan superhost tetapi sebenarnya adalah superhost.
> * `False Positive` = Host yang teridentifikasi sebagai superhost tetapi sebenarnya bukan superhost.

**Parameter Evaluasi Model**

- `Precision` sebagai Parameter Evaluasi Utama

- Meningkatkan akurasi dalam mengidentifikasi superhost.  
- Mereduksi False Positif (Host yang diprediksi sebagai superhost, namun kenyataannya bukan).

- `Recall` sebagai Parameter Evaluasi Sekunder

- Mengoptimalkan jumlah superhost yang teridentifikasi.  
- Mereduksi False Negative (Host yang diprediksi bukan superhost, namun sebenarnya adalah superhost).

- `F1 Score` untuk memeriksa keseimbangan antara positif dan negatif (Data Tidak Seimbang). Sangat penting untuk masalah ini, kita membutuhkan trade-off antara Precision dan Recall, oleh karena itu, kami menggunakan skor F1 sebagai metrik. Skor F1 didefinisikan sebagai rata-rata harmonik dari Precision dan Recall.

**Interpretation**

- `Precision` â€“ Berapa persen dari prediksi yang benar? :

Dari test data yang diprediksi positif (superhost), berarti x% yang sebenarnya adalah positif (superhost)

Dari data yang diprediksi sebagai superhost, berarti x% yang sebenarnya adalah superhost.

- `Recall` â€“ Berapa persen dari kasus positif yang berhasil kita tangkap? :

Dari semua yang sebenarnya adalah positif (superhost), yang berhasil diprediksi sebagai superhost adalah x%.

**`Target Metrics` =  `Precision` / `Recall` & `F1 Score`**

- `Precision` adalah rasio prediksi benar positif dibandingkan dengan keseluruhan hasil yang diprediksi positif. Pilih algoritma yang memiliki `Precision` tinggi, jika skenario yang dipilih adalah `False Negative lebih baik terjadi daripada False Positif`.

$$  
\text{Precision} = \frac{TP}{TP + FP}  
$$

- `Recall` adalah rasio kasus dengan prediksi benar positif dibandingkan dengan keseluruhan data yang benar positif. Pilih algoritma yang memiliki `Recall` tinggi, jika skenario yang dipilih adalah `False Positive lebih baik terjadi daripada False Negative`.

$$  
\text{Recall} = \frac{TP}{TP + FN}  
$$

- `F1-Score` atau dikenal juga dengan nama F-Measure didapatkan dari perbandingan rata-rata presisi dengan recall yang dibobotkan.

$$  
\text{F1 Score} = \frac{2 \times (Recall \times Precision)}{Recall + Precision}  
$$

**Description Classification Report :**

Ini adalah metode Python di bawah API metrik sklearn, berguna ketika kita membutuhkan metrik per kelas di samping metrik global. Ini memberikan precision, recall, dan F1 score pada tingkat individu dan global.

* `precision`akan menjadi "berapa banyak yang diklasifikasikan dengan benar di antara kelas tersebut" (Persentase prediksi positif yang benar relatif terhadap total prediksi positif).

* `recall` berarti "berapa banyak dari kelas ini yang Anda temukan di seluruh jumlah elemen kelas ini". (Persentase prediksi positif yang benar relatif terhadap total aktual positif).

* `f1-score` adalah rata-rata harmonik antara precision & recall. Semakin dekat ke 1, semakin baik model.

* `support` adalah jumlah kemunculan kelas tertentu dalam dataset. Nilai-nilai ini memberi tahu kita berapa banyak host yang termasuk dalam setiap kelas dalam dataset uji. (untuk memeriksa dataset yang seimbang/Proporsi).

* `Accuracy` adalah jumlah dari true positives dan true negatives dibandingkan dengan total jumlah sampel.

* `Macro Average` adalah rata-rata dari precision/recall/F1-score dari semua kelas.

* `Weighted Average` menghitung skor untuk setiap kelas secara independen, tetapi ketika dijumlahkan, mempertimbangkan jumlah klasifikasi yang benar dari setiap kelas.

---

**Referensi Teori**

- https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html
- https://muthu.co/understanding-the-classification-report-in-sklearn/#:~:text=A%20Classification%20report%20is%20used,classification%20report%20as%20shown%20below.
- https://machinelearningmastery.com/fbeta-measure-for-machine-learning/
- https://rey1024.medium.com/mengenal-accuracy-precission-recall-dan-specificity-serta-yang-diprioritaskan-b79ff4d77de8
- https://www.statology.org/sklearn-classification-report/

## **ðŸ“Œ Modelling**
"""

from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
import xgboost as xgb
import lightgbm as lgb

# Modelling

models = {
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42),
    "Logistic Regression": LogisticRegression(random_state=42),
    "Gaussian Naive Bayes": GaussianNB(),
    "K-Nearest Neighbor": KNeighborsClassifier(),
    "MLP Classifier": MLPClassifier(random_state=42, max_iter=1000),
    "Adaboost Classifier": AdaBoostClassifier(random_state=42),
    "Gradient Boosting Classifier": GradientBoostingClassifier(random_state=42),
    "XGBoost Classifier": xgb.XGBClassifier(random_state=42, use_label_encoder=False),
    "LGBM Classifier": lgb.LGBMClassifier(random_state=42),
    "Support Vector Machine": SVC(random_state=42)
}

results = {}

param_grids = {
    # "Decision Tree": {
    #     "criterion": ["gini", "entropy"],
    #     "max_depth": [None, 10, 20, 30],
    #     "min_samples_split": [2, 5, 10],
    # },
    # "Random Forest": {
    #     "n_estimators": [50, 100, 200],
    #     "max_depth": [None, 10, 20],
    #     "min_samples_split": [2, 5, 10],
    # },
    # "Logistic Regression": {
    #     "C": [0.01, 0.1, 1, 10],
    #     "solver": ["liblinear", "lbfgs"],
    # },
    # "Gaussian Naive Bayes": {},  # No hyperparameters to tune
    # "K-Nearest Neighbor": {
    #     "n_neighbors": [3, 5, 7, 9],
    #     "weights": ["uniform", "distance"],
    #     "metric": ["euclidean", "manhattan"],
    # },
    # "MLP Classifier": {
    #     "hidden_layer_sizes": [(50,), (100,), (50, 50)],
    #     "activation": ["relu", "tanh"],
    #     "solver": ["adam", "sgd"],
    #     "alpha": [0.0001, 0.001],
    # },
    # "Adaboost Classifier": {
    #     "n_estimators": [50, 100, 200],
    #     "learning_rate": [0.01, 0.1, 1],
    # },
    # "XGBoost Classifier": {
    #     "n_estimators": [50, 100, 200],
    #     "learning_rate": [0.01, 0.1, 0.2],
    #     "max_depth": [3, 6, 10],
    # },
    # "LGBM Classifier": {
    #     "n_estimators": [50, 100, 200],
    #     "learning_rate": [0.01, 0.1, 0.2],
    #     "max_depth": [3, 6, 10],
    # },


    # Sumber Parameter
    # https://www.researchgate.net/publication/372588758_A_fine_tune_hyper_parameter_Gradient_Boosting_model_for_CPU_utilization_prediction_in_cloud_environment

    #     'n_estimators': [50, 100, 200],
    # 'learning_rate': [0.01, 0.1, 0.2],
    # 'max_depth': [3, 5, 7],

    # https://www.geeksforgeeks.org/understanding-feature-importance-and-visualization-of-tree-models/

    # "Gradient Boosting Classifier": {
    #       'n_estimators': [50, 100, 200],
    #       'learning_rate': [0.01, 0.1, 0.2, 0.9],
    #       'max_depth': [3, 5, 7],
    #       'min_samples_split': [2, 10],
    #       'min_samples_leaf': [1, 6],
    # },


    # "Support Vector Machine": {
    #     "C": [0.1, 1, 10],
    #     "kernel": ["linear", "rbf", "poly"],
    #     "gamma": ["scale", "auto"],
    # },
}

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.metrics import accuracy_score, classification_report
from sklearn.metrics import precision_score, recall_score, f1_score
import time
import pickle



def model_training(X_train, X_test, y_train, y_test, models, param_grids=None, fold_validation=True, cv_folds=5, saved_models=True):
    results = {}

    for name, model in models.items():
        print(f"\nTraining {name}...")
        start_time = time.time()

        if param_grids and name in param_grids and fold_validation:
            print(f"Performing GridSearchCV for {name} with fold validation...")
            grid_search = GridSearchCV(
                model, param_grids[name], cv=StratifiedKFold(n_splits=cv_folds),
                scoring="accuracy", n_jobs=-1
            )
            grid_search.fit(X_train, y_train)
            best_model = grid_search.best_estimator_
            best_params = grid_search.best_params_
            model_to_evaluate = best_model
        else:
            if fold_validation:
                print(f"Performing fold validation for {name} without grid search...")
                skf = StratifiedKFold(n_splits=cv_folds)
                train_acc_list, test_acc_list = [], []

                for train_index, val_index in skf.split(X_train, y_train):
                    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
                    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

                    model.fit(X_train_fold, y_train_fold)
                    train_acc_list.append(accuracy_score(y_train_fold, model.predict(X_train_fold)))
                    test_acc_list.append(accuracy_score(y_val_fold, model.predict(X_val_fold)))

                train_acc = sum(train_acc_list) / len(train_acc_list)
                test_acc = sum(test_acc_list) / len(test_acc_list)
                model.fit(X_train, y_train)
                model_to_evaluate = model
            else:
                print(f"Training {name} without grid search or fold validation...")
                model.fit(X_train, y_train)
                model_to_evaluate = model

        # Evaluate the model
        y_train_pred = model_to_evaluate.predict(X_train)
        y_test_pred = model_to_evaluate.predict(X_test)

        train_acc = accuracy_score(y_train, y_train_pred)
        test_acc = accuracy_score(y_test, y_test_pred)
        train_precision = precision_score(y_train, y_train_pred, average='weighted')
        test_precision = precision_score(y_test, y_test_pred, average='weighted')
        train_recall = recall_score(y_train, y_train_pred, average='weighted')
        test_recall = recall_score(y_test, y_test_pred, average='weighted')
        train_f1 = f1_score(y_train, y_train_pred, average='weighted')
        test_f1 = f1_score(y_test, y_test_pred, average='weighted')

        # Calculate confusion matrix
        cm = confusion_matrix(y_test, y_test_pred)

        training_time = time.time() - start_time

        results[name] = {
            "train_accuracy": train_acc,
            "test_accuracy": test_acc,
            "train_precision": train_precision,
            "test_precision": test_precision,
            "train_recall": train_recall,
            "test_recall": test_recall,
            "train_f1": train_f1,
            "test_f1": test_f1,
            "training_time": training_time,
            "classification_report": classification_report(y_test, y_test_pred, output_dict=True),
            "confusion_matrix": cm,
            "model": model_to_evaluate
        }

        # Plot confusion matrix
        plt.figure(figsize=(6, 4))
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False,
                    xticklabels=model_to_evaluate.classes_, yticklabels=model_to_evaluate.classes_)
        plt.title(f"Confusion Matrix for {name}")
        plt.xlabel("Predicted")
        plt.ylabel("Actual")
        plt.show()

        try:
            from sklearn.metrics import precision_recall_curve

            y_test_prob = model.predict_proba(X_test)[:, 1]
            precisions, recalls, thresholds = precision_recall_curve(y_test, y_test_prob)
            # Evaluate F1-scores at each threshold
            f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-9)  # add small epsilon to avoid division by zero
            best_idx = np.argmax(f1_scores)
            best_threshold = thresholds[best_idx]
            print("Best Threshold for F1:", best_threshold)
        except:
            print("Unecessary Error")

        # Uncomment Code Below to Save the model in your Google Drive
        # from google.colab import drive

        # drive.mount('/content/drive')

        # if saved_models:
        #     filename = f'/content/drive/MyDrive/Models/{name}.pkl'
        #     pickle.dump(model_to_evaluate, open(filename, 'wb'))

    return results

# Example usage
results = model_training(
    X_train, X_test, y_train, y_test,
    models=models,
    param_grids=param_grids,  # Optional
    fold_validation=True,     # Set to False to disable fold validation
    cv_folds=10,              # Number of folds for cross-validation
    saved_models=True         # Set to False to disable saving models
)

"""#### **Model Evaluation**"""

# Print results
for name, res in results.items():
    print(f"\nModel: {name}")
    if "best_params" in res:
        print(f"Best Parameters: {res['best_params']}")
    print(f"Train Accuracy: {res['train_accuracy']:.4f}")
    print(f"Test Accuracy: {res['test_accuracy']:.4f}")
    print(f"Train Precision: {res['train_precision']:.4f}")
    print(f"Test Precision: {res['test_precision']:.4f}")
    print(f"Train Recall: {res['train_recall']:.4f}")
    print(f"Test Recall: {res['test_recall']:.4f}")
    print(f"Train F1 Score: {res['train_f1']:.4f}")
    print(f"Test F1 Score: {res['test_f1']:.4f}")
    print(f"Training Time: {res['training_time']:.2f} seconds")
    print("Classification Report:")
    print(classification_report(y_test, res["model"].predict(X_test)))
    print("Confusion Matrix:")
    print(res["confusion_matrix"])
    print("=" * 60)

"""#### **Performance of Training Model**"""

import pandas as pd

# Create a DataFrame to store the results
output_data = []

for name, res in results.items():
    row = {
        "Model": name,
        "Train Accuracy": res["train_accuracy"],
        "Test Accuracy": res["test_accuracy"],
        "Train Precision": res["train_precision"],
        "Test Precision": res["test_precision"],
        "Train Recall": res["train_recall"],
        "Test Recall": res["test_recall"],
        "Train F1 Score": res["train_f1"],
        "Test F1 Score": res["test_f1"],
        # "Training Time": res["training_time"]
    }
    output_data.append(row)

# Convert the list of dictionaries to a DataFrame
output_df = pd.DataFrame(output_data)

# Set the Model column as the index
output_df.set_index("Model", inplace=True)

# Function to apply color gradient (dark blue to light blue)
def color_gradient(val):
    normalized_val = val / output_df.max().max()
    color = f"background-color: rgba(173, 216, 230, {normalized_val}); color: black"
    return color

# Apply the color gradient to the DataFrame
styled_df = (
    output_df.style
    .applymap(color_gradient)  # Apply color gradient
    .format("{:.2f}")  # Format all values to 2 decimal places
)


# Display the styled DataFrame
display(styled_df)

"""### **Comparison Time Training**"""

import pandas as pd

comparison_data = []

for name, res in results.items():
    row = {
        "Model": name,
        "Train Accuracy": res["train_accuracy"],
        "Test Accuracy": res["test_accuracy"],
        "Training Time": res["training_time"],
        "Precision (Train)": res["classification_report"]["weighted avg"]["precision"],
        "Recall (Train)": res["classification_report"]["weighted avg"]["recall"],
        "F1 Score (Train)": res["classification_report"]["weighted avg"]["f1-score"],
        "Precision (Test)": res["classification_report"]["weighted avg"]["precision"],
        "Recall (Test)": res["classification_report"]["weighted avg"]["recall"],
        "F1 Score (Test)": res["classification_report"]["weighted avg"]["f1-score"]
    }
    comparison_data.append(row)

# Create DataFrame
comparison_df = pd.DataFrame(comparison_data)

# Set the Model column as the index
comparison_df.set_index("Model", inplace=True)

# Display the DataFrame
print(comparison_df)

"""### **Feature Importance**"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.inspection import permutation_importance

def plot_feature_importance(model, X_train, y_train):
    # Calculate permutation importance
    result = permutation_importance(model, X_train, y_train, n_repeats=10, random_state=42, n_jobs=-1)

    # Create a DataFrame
    importance_df = pd.DataFrame({
        'Feature': X_train.columns,
        'Importance': result.importances_mean
    })
    importance_df = importance_df.sort_values(by='Importance', ascending=False)

    # Plotting
    plt.figure(figsize=(10, 6))
    bar_plot = sns.barplot(x='Importance', y='Feature', data=importance_df)
    plt.title('Feature Importance')

    # Adding the importance values on top of the bars
    for index, value in enumerate(importance_df['Importance']):
        bar_plot.text(value, index, f'{value:.4f}', color='black', ha="left", va="center")

    plt.show()

# Assuming results is a dictionary containing models
for name, res in results.items():
    print(f"Feature Importance for Model: {name}")
    plot_feature_importance(res['model'], X_train, y_train)

"""## **ðŸ“Œ Model Selection**

**Model: Gradient Boosting Classifier**

*  Train Accuracy: 0.8443
*  Test Accuracy: 0.7887
*  Train Precision: 0.8420
*  Test Precision: 0.7765
*  Train Recall: 0.8443
*  Test Recall: 0.7887
*  Train F1 Score: 0.8412
*  Test F1 Score: 0.7801
*  Training Time: 9.80 seconds
Classification Report:
              precision    recall  f1-score   support

           0       0.84      0.89      0.86       479
           1       0.60      0.47      0.53       160

    accuracy                           0.79       639
   macro avg       0.72      0.68      0.70       639
weighted avg       0.78      0.79      0.78       639

Confusion Matrix:
[[428  51]
 [ 84  76]]

### **Summary**

Berdasarkan hasil temuan dengan menggunakan berbagai macam model, ditemukan model terbaik yaitu :

**`Gradient Boosting Classifier`**

dengan method sebagai berikut :

* `Default Parameter (No Tuning)`
Dengan parameter terbaik menggunakan Grid Search hasilnya menjadi overfitting pada modelnya.
* `Oversampling (randomoversampler)`
Menambah 2% akurasi Test

* `23 Feature, 1 Target`
* `Label Encoder`
Untuk fitur Categorical

* `Yeo-Johnson Transformation`
Menambah 2% akurasi Gradient Boosting Classifier
Menambah 0-2% akurasi model lainnya.

* `Robust Scaler`

## **ðŸ“Œ Business Insight and Recommendation**

**ðŸ“ŒConclusion**

Berdasarkan modelling machine learning menggunakan metode `Graidient Boosting Clasiffier` dengan hasil sebagai berikut :

    - Precision = 0.78
    - Recall = 0.79
    - F1 Score = 0.78
    - Training Time : 12.21 seconds
    * TP = 76
    * TN = 428
    * FP = 51
    * FN = 84

Didapatkan Top feature yang paling berpengaruh untuk memprediksi model terhadap target/label `host_is_superhost` diantaranya adalah:

   1. review_scores_cleanliness
Skor kebersihan properti
   2. space
Deskripsi tentang ruang akomodasi
   3. weekly_price
Harga sewa mingguan
   4. review_scores_accuracy
Skor akurasi deskripsi listing  
   5. Amenities
Fasilitas yang lengkap dan berkualitas
   6. review_score_value
Skor nilai dari ulasan
   7. review_score_communication
Skor komunikasi dari ulasan.
   8. host_identity_verified
 Verifikasi identitas host

## ðŸ“ŒStrategic Action
1. **Rekomendasi : Fokus pada Peningkatan Kualitas Properti & Pengalaman Tamu**
* Insight : Skor kebersihan memiliki nilai tertinggi dan sangat memengaruhi status Superhost. Meskipun banyak properti sudah bersih, ada peluang untuk meningkatkan kepercayaan tamu melalui akurasi deskripsi dan menyediakan fasilitas yang lengkap untuk pengalaman yang lebih menarik.

* Actionable Items :
  1. Implement Regular Cleaning Protocols: Lakukan audit kebersihan secara berkala untuk memastikan standar tinggi dan konsistensi.
  2. Enhance Listing Descriptions: Perbarui deskripsi properti agar lebih akurat dan mencerminkan semua fasilitas yang tersedia.
  3. Upgrade Amenities: Tambahkan fasilitas yang menarik dan berkualitas untuk meningkatkan daya tarik properti dan pengalaman tamu.


2. **Rekomendasi : Manfaatkan Peningkatan Komunikasi dan Kepercayaan Tamu
Insight**

* Insight : Skor komunikasi dan verifikasi identitas host sangat penting untuk membangun kepercayaan tamu. Ada peluang untuk meningkatkan kepercayaan dengan memperkuat komunikasi interaktif dan memastikan identitas host terverifikasi.

* Actionable Items :
  1. Implement Proactive Communication Strategies: Dorong host untuk berkomunikasi secara proaktif dengan tamu sebelum dan selama masa inap mereka.
  2. Enhance Identity Verification Process: Pastikan semua host menjalani verifikasi identitas yang ketat untuk meningkatkan kepercayaan.
  3. Collect Feedback on Communication: Kumpulkan umpan balik tamu tentang pengalaman komunikasi untuk mengidentifikasi area perbaikan.

3. **Rekomendasi : Optimalkan Penetapan Harga untuk Daya Tarik Maksimal**

* insight : Harga sewa mingguan berpengaruh pada keputusan tamu. Penetapan harga yang kompetitif dapat menarik lebih banyak pemesanan.

* Actionable Items:
  1. Gather Guest Feedback for Pricing : Kumpulkan umpan balik dari tamu mengenai harga dan nilai yang mereka terima, dan diskusikan dengan host untuk menyesuaikan harga sewa mingguan agar lebih kompetitif dan sesuai dengan ekspektasi tamu.
  2. Promote Special Events: bekerja sama dengan host untuk mempromosikan acara lokal atau musiman, dan sesuaikan harga sewa untuk menarik tamu selama periode tersebut.
  3. Conduct Joint Promotions: Ajak host untuk berkolaborasi dalam promosi khusus, seperti diskon untuk pemesanan di akhir pekan atau selama musim liburan,

###Distribusi Nilai Kebersihan Ulasan Berdasarkan Status Superhost
"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8, 6))

colors = ["#ffac18", "#c43623"]

custom_palette = sns.color_palette(colors)

ax = sns.barplot(
    x='host_is_superhost',
    y='review_scores_cleanliness',
    data=df,
    palette=custom_palette
)

plt.title('Distribusi Nilai Kebersihan Ulasan Berdasarkan Status Superhost', fontsize=14, fontweight='bold')
plt.xlabel('Status Superhost', fontsize=12)
plt.ylabel('Nilai Kebersihan', fontsize=12)

plt.xticks([0, 1], ['Host', 'Superhost'], fontsize=10)

plt.gca().invert_yaxis()

plt.grid(axis='y', linestyle='--', alpha=0.7)

sns.despine()

for p in ax.patches:
    ax.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),
                textcoords='offset points')

plt.tight_layout()
plt.show()

"""Diagram ini menunjukkan bahwa properti dengan status Superhost memiliki nilai rata-rata kebersihan ulasan yang lebih tinggi (lebih dekat ke 0) dibandingkan dengan properti yang bukan Superhost. Ini mengindikasikan bahwa tamu cenderung memberikan ulasan yang lebih baik tentang kebersihan properti Superhost dibandingkan dengan properti yang bukan Superhost.

###Distribusi Fasilitas yang Lengkap dan Berkualitas Status Superhost
"""

import matplotlib.pyplot as plt
import seaborn as sns

sns.set_style("whitegrid")

plt.figure(figsize=(10, 6))
ax = sns.barplot(
    x='host_is_superhost',
    y='amenities',
    data=df,
    palette=["#ffac18", "#c43623"],
    ci=None,
    edgecolor=".2"
)

plt.title(
    'Distribusi Fasilitas yang Lengkap dan Berkualitas Status Superhost',
    fontsize=16,
    fontweight='bold'
)
plt.xlabel('Status Superhost', fontsize=14)
plt.ylabel('Amenities', fontsize=14)
plt.xticks([0, 1], ['Host', 'Superhost'], fontsize=12)

for p in ax.patches:
    ax.annotate(
        f'{p.get_height():.2f}',
        (p.get_x() + p.get_width() / 2., p.get_height()),
        ha='center',
        va='center',
        fontsize=12,
        color='black',
        xytext=(0, 5),
        textcoords='offset points'
    )

sns.despine()

plt.grid(axis='y', linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()

"""Diagram ini menunjukkan bahwa properti dengan status Superhost cenderung memiliki fasilitas yang lebih lengkap dan berkualitas dibandingkan dengan properti yang bukan Superhost. Perbedaan nilai antara keduanya (1465.85 vs 1382.40) mengindikasikan bahwa Superhost mungkin menawarkan lebih banyak fasilitas atau fasilitas yang lebih baik kepada tamu mereka.

###Distribusi Skor Akurasi Deskripsi Listing Berdasarkan Status Superhost
"""

sns.set_style("whitegrid")

plt.figure(figsize=(10, 6))

plt.figure(figsize=(8, 6))

colors = ["#ffac18", "#c43623"]

custom_palette = sns.color_palette(colors)

ax = sns.barplot(
    x='host_is_superhost',
    y='review_scores_accuracy',
    data=df,
    palette=custom_palette
)

plt.title(
    'Distribusi Skor Akurasi Deskripsi Listing Berdasarkan Status Superhost',
    fontsize=16,
    fontweight='bold'
)

plt.xlabel('Status Superhost', fontsize=14)
plt.ylabel('Skor Akurasi', fontsize=14)

plt.xticks([0, 1], ['Host', 'Superhost'], fontsize=12)

for p in ax.patches:
    ax.annotate(
        f'{p.get_height():.2f}',
        (p.get_x() + p.get_width() / 2., p.get_height()),
        ha='center',
        va='center',
        fontsize=12,
        color='black',
        xytext=(0, 5),
        textcoords='offset points'
    )

sns.despine()

plt.grid(axis='y', linestyle='--', alpha=0.7)

plt.gca().invert_yaxis()

plt.tight_layout()
plt.show()

"""Diagram ini menunjukkan bahwa properti dengan status Superhost memiliki skor akurasi deskripsi listing yang lebih tinggi (lebih dekat ke 0) dibandingkan dengan properti yang bukan Superhost. Ini mengindikasikan bahwa deskripsi listing properti Superhost cenderung lebih akurat dan sesuai dengan kondisi aktual properti.

###Distribusi Weekly Price Berdasarkan Status Superhost

###Distribusi Skor Nilai dari Ulasan Berdasarkan Status Superhost
"""

sns.set_style("whitegrid")

plt.figure(figsize=(8, 6))

colors = ["#ffac18", "#c43623"]

custom_palette = sns.color_palette(colors)

ax = sns.barplot(
    x='host_is_superhost',
    y='weekly_price',
    data=df,
    palette=custom_palette
)


plt.title(
    'Distribusi Weekly Price Berdasarkan Status Superhost',
    fontsize=16,
    fontweight='bold'
)

plt.xlabel('Status Superhost', fontsize=14)
plt.ylabel('Weekly Price', fontsize=14)

plt.xticks([0, 1], ['Host', 'Superhost'], fontsize=12)

for p in ax.patches:
    ax.annotate(
        f'{p.get_height():.2f}',
        (p.get_x() + p.get_width() / 2., p.get_height()),
        ha='center',
        va='center',
        fontsize=12,
        color='black',
        xytext=(0, 5),
        textcoords='offset points'
    )

sns.despine()

plt.grid(axis='y', linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()

"""Diagram ini menunjukkan bahwa properti dengan status Superhost memiliki harga mingguan yang lebih tinggi (0.06) dibandingkan dengan properti yang bukan Superhost (-0.02). Ini mengindikasikan bahwa properti Superhost cenderung mengenakan biaya lebih untuk sewa mingguan mereka."""

sns.set_style("whitegrid")

plt.figure(figsize=(8, 6))

colors = ["#ffac18", "#c43623"]

custom_palette = sns.color_palette(colors)

ax = sns.barplot(
    x='host_is_superhost',
    y='review_scores_value',
    data=df,
    palette=custom_palette
)


plt.title(
    'Distribusi Skor Nilai dari Ulasan Berdasarkan Status Superhost',
    fontsize=16,
    fontweight='bold'
)

plt.xlabel('Status Superhost', fontsize=14)
plt.ylabel('Skor nilai dari ulasan', fontsize=14)

plt.xticks([0, 1], ['Host', 'Superhost'], fontsize=12)

for p in ax.patches:
    ax.annotate(
        f'{p.get_height():.2f}',
        (p.get_x() + p.get_width() / 2., p.get_height()),
        ha='center',
        va='center',
        fontsize=12,
        color='black',
        xytext=(0, 5),
        textcoords='offset points'
    )

sns.despine()

plt.grid(axis='y', linestyle='--', alpha=0.7)

plt.gca().invert_yaxis()

plt.tight_layout()
plt.show()

"""Diagram ini menunjukkan bahwa properti dengan status Superhost memiliki skor nilai rata-rata ulasan yang lebih tinggi (lebih dekat ke 0) dibandingkan dengan properti yang bukan Superhost. Ini mengindikasikan bahwa tamu cenderung memberikan nilai yang lebih baik pada properti Superhost dibandingkan dengan properti yang bukan Superhost.

###Distribusi Ruang Akomodasi Berdasarkan Status Superhost
"""

sns.set_style("whitegrid")

plt.figure(figsize=(8, 6))

colors = ["#ffac18", "#c43623"]

custom_palette = sns.color_palette(colors)

ax = sns.barplot(
    x='host_is_superhost',
    y='space',
    data=df,
    palette=custom_palette
)


plt.title(
    'Distribusi Ruang Akomodasi Berdasarkan Status Superhost',
    fontsize=16,
    fontweight='bold'
)

plt.xlabel('Status Superhost', fontsize=14)
plt.ylabel('Ruang Akomodasi', fontsize=14)

plt.xticks([0, 1], ['Host', 'Superhost'], fontsize=12)

for p in ax.patches:
    ax.annotate(
        f'{p.get_height():.2f}',
        (p.get_x() + p.get_width() / 2., p.get_height()),
        ha='center',
        va='center',
        fontsize=12,
        color='black',
        xytext=(0, 5),
        textcoords='offset points'
    )

sns.despine()

plt.grid(axis='y', linestyle='--', alpha=0.7)


plt.tight_layout()
plt.show()

"""Diagram ini menunjukkan bahwa properti dengan status Superhost cenderung memiliki ruang akomodasi yang lebih besar atau lebih banyak dibandingkan dengan properti yang bukan Superhost. Perbedaan nilai antara keduanya (1331.86 vs 1197.86) mengindikasikan bahwa Superhost mungkin menawarkan lebih banyak kamar, properti yang lebih luas, atau kapasitas hunian yang lebih besar kepada tamu mereka.

###Distribusi Skor Komunikasi dari Ulasan Berdasarkan Status Superhost
"""

sns.set_style("whitegrid")

plt.figure(figsize=(8, 6))

colors = ["#ffac18", "#c43623"]

custom_palette = sns.color_palette(colors)

ax = sns.barplot(
    x='host_is_superhost',
    y='review_scores_communication',
    data=df,
    palette=custom_palette
)


plt.title(
    'Distribusi Skor Komunikasi dari Ulasan Berdasarkan Status Superhost',
    fontsize=16,
    fontweight='bold'
)

plt.xlabel('Status Superhost', fontsize=14)
plt.ylabel('Skor Komunikasi dari Ulasan', fontsize=14)

plt.xticks([0, 1], ['Host', 'Superhost'], fontsize=12)

for p in ax.patches:
    ax.annotate(
        f'{p.get_height():.2f}',
        (p.get_x() + p.get_width() / 2., p.get_height()),
        ha='center',
        va='center',
        fontsize=12,
        color='black',
        xytext=(0, 5),
        textcoords='offset points'
    )

sns.despine()

plt.grid(axis='y', linestyle='--', alpha=0.7)

plt.gca().invert_yaxis()

plt.tight_layout()
plt.show()

"""Superhost memiliki komunikasi yang lebih baik dibandingkan Host biasa, yang kemungkinan menjadi salah satu faktor mereka mendapatkan status Superhost.

###Distribusi Tipe Properti Berdasarkan Status Superhost
"""

sns.set_style("whitegrid")

plt.figure(figsize=(8, 6))

colors = ["#ffac18", "#c43623"]

custom_palette = sns.color_palette(colors)

ax = sns.barplot(
    x='host_is_superhost',
    y='property_type',
    data=df,
    palette=custom_palette,
    estimator=len
)


plt.title(
    'Distribusi Tipe Property Berdasarkan Status Superhost',
    fontsize=16,
    fontweight='bold'
)

plt.xlabel('Status Superhost', fontsize=14)
plt.ylabel('Tipe Properti', fontsize=14)

plt.xticks([0, 1], ['Host', 'Superhost'], fontsize=12)

for p in ax.patches:
    height = p.get_height()
    ax.annotate(
        f'{int(height)}',
        (p.get_x() + p.get_width() / 2., height),
        ha='center', va='center',
        fontsize=12, color='black',
        xytext=(0, 5),
        textcoords='offset points'
    )

sns.despine()

plt.grid(axis='y', linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()

"""Dari data ini, dapat disimpulkan bahwa jumlah properti yang dimiliki oleh Host biasa jauh lebih banyak dibandingkan dengan yang dimiliki oleh Superhost. Superhost adalah kategori khusus dalam platform seperti Airbnb yang diberikan kepada tuan rumah dengan ulasan tinggi dan layanan unggul. Meski jumlahnya lebih sedikit, mereka cenderung memiliki kualitas layanan yang lebih baik."""

